{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f04034",
   "metadata": {},
   "source": [
    "# **Dimensionality Reduction Methods**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baac4250",
   "metadata": {},
   "source": [
    "\n",
    "Dimensionality of a dataset refers to the number of features within a dataset and reducing dimensionality allows for faster runtimes and (often) better performance. This is an extremely powerful tool in working with datasets with “high dimensionality”. For instance, a hundred-feature problem can be reduced to less than ten modified features, saving a lot of computational time and resources while maintaining or even improving performance. Typically, dimensionality reduction methods are machine learning algorithms themselves, such as Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), etc.\n",
    "\n",
    "These techniques transform the existing feature space into a new subset of features that are ordered by decreasing importance. Since they “extract” new features from high dimensional data they’re also referred to as Feature Extraction methods. The transformed features do not directly relate to anything in the real world anymore. Rather, they are mathematical objects that are related to the original features. However, these mathematical objects are often difficult to interpret. The lack of interpretability is one of the drawbacks of dimensionality reduction.\n",
    "\n",
    "We can apply these reduction methods not just to the whole feature space `X` but to a portion of highly corellated features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f98ecb",
   "metadata": {},
   "source": [
    "## **Principal Component Analysis (PCA)**\n",
    "\n",
    "Principal component analysis (PCA) is an unsupervised learning algorithm that transforms high-dimensional data into a smaller number of features using principal components (PCs). The goal is to preserve as much variance and information from the original data in a lower-dimensional space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85ad8f2",
   "metadata": {},
   "source": [
    "### **np.linalg.eig**\n",
    "\n",
    "The new features generated by PCA are linear combinations of the original features. In this exercise, we will perform PCA using the NumPy method `np.linalg.eig`, which performs eigen decomposition and outputs the eigenvalues and eigenvectors. The eigenvalues are related to the relative variation described by each principal component. The eigenvectors are also known as the principal axes. They tell us how to transform (rotate) our data into new features that capture this variation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d09cf6",
   "metadata": {},
   "source": [
    "```python\n",
    "correlation_matrix = data.corr()\n",
    "eigenvalues, eigenvectors = np.linalg.eig(correlation_matrix)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59243f99",
   "metadata": {},
   "source": [
    "After performing PCA, we generally want to know how useful the new features are. One way to visualize this is to create a scree plot, which shows the proportion of information described by each principal component. The proportion of information explained is equal to the relative size of each eigenvalue:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bba826f",
   "metadata": {},
   "source": [
    "```python\n",
    "info_prop = eigenvalues / eigenvalues.sum()\n",
    "print(info_prop)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82fb5e4e",
   "metadata": {},
   "source": [
    "To create a scree plot, we can then plot these relative proportions:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b1b8bc",
   "metadata": {},
   "source": [
    "```python\n",
    "plt.plot(np.arange(1,len(info_prop)+1), info_prop, 'bo-')\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467e8d5",
   "metadata": {},
   "source": [
    "Another way to view this is to see how many principal axes it takes to reach around 95% of the total amount of information. Ideally, we’d like to retain as few features as possible while still reaching this threshold. To do this, we need to calculate the cumulative sum of the info_prop vector we created earlier:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9ce0e7",
   "metadata": {},
   "source": [
    "```python\n",
    "cum_info_prop = np.cumsum(info_prop)\n",
    "\n",
    "plt.plot(np.arange(1,len(info_prop)+1), cum_info_prop, 'bo-')\n",
    "plt.hlines(y=.95, xmin=0, xmax=15)\n",
    "plt.vlines(x=4, ymin=0, ymax=1)\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18e962c",
   "metadata": {},
   "source": [
    "### **sklearn.decomposition.PCA**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27cb1acb",
   "metadata": {},
   "source": [
    "Another way to perform PCA is using the scikit-learn module `sklearn.decomposition.PCA`. The steps to perform PCA using this method are:\n",
    "\n",
    "1. Standardize the data matrix. This is done by subtracting the mean and dividing by the standard deviation of each column vector.\n",
    "\n",
    "```python\n",
    "    # Standardize numerical features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "```\n",
    "\n",
    "2. Perform eigendecomposition by fitting the standardized data. We can access the eigenvectors using the components_ attribute and the proportional sizes of the eigenvalues using the `explained_variance_ratio_` attribute. This module has many advantages over the NumPy method, including a number of different solvers to calculate the principal axes. This can greatly improve the quality of the results.\n",
    "\n",
    "```python\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "\n",
    "    components = pca.fit(data_standardized).components_\n",
    "    components = pd.DataFrame(components).transpose()\n",
    "    components.index =  data_matrix.columns\n",
    "    print(components)\n",
    "\n",
    "    var_ratio = pca.explained_variance_ratio_\n",
    "    var_ratio = pd.DataFrame(var_ratio).transpose()\n",
    "    print(var_ratio)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2ab6a1",
   "metadata": {},
   "source": [
    "3. Once we have performed PCA and obtained the eigenvectors, we can use them to project the data onto the first few principal axes. We can do this by taking the dot product of the data and eigenvectors, or by using the `sklearn.decomposition.PCA` module as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31558e1f",
   "metadata": {},
   "source": [
    "```python\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    # Apply PCA (reduce to 2 components for visualization)\n",
    "    pca = PCA(n_components=2)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "    # Convert to DataFrame for visualization\n",
    "    df_pca = pd.DataFrame(X_pca, columns=['PC1', 'PC2'])\n",
    "    df_pca['Survived'] = y.values\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94a4dca3",
   "metadata": {},
   "source": [
    "4. Once we have the transformed data, we can look at a scatter plot of the first two transformed features using seaborn or matplotlib. This allows us to view relationships between multiple features at once in 2D or 3D space. Often, the the first 2-3 principal components result in clustering of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe324eac",
   "metadata": {},
   "source": [
    "```python\n",
    "    # Plot PCA results\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.scatterplot(x='PC1', y='PC2', hue='Survived', data=df_pca, palette='coolwarm', alpha=0.7)\n",
    "    plt.title('PCA on Titanic Dataset')\n",
    "    plt.xlabel('Principal Component 1')\n",
    "    plt.ylabel('Principal Component 2')\n",
    "    plt.legend(title='Survived')\n",
    "    plt.show()\n",
    "\n",
    "    sns.lmplot(x='PC1', y='PC2', data=df_pca, hue='species', fit_reg=False)\n",
    "    plt.show()\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e7e979",
   "metadata": {},
   "source": [
    "So far we have used PCA to find principal axes and project the data onto them. We can use a subset of the projected data for modeling, while retaining most of the information in the original (and higher-dimensional) dataset.\n",
    "\n",
    "For example, recall in the previous exercise that the first four principal axes already contained 95% of the total amount of variance (or information) in the original data. We can use the first four components to train a model, just like we would on the original 16 features.\n",
    "\n",
    "Because of the lower dimensionality, we should expect training times to be faster. Furthermore, the principal axes ensure that each new feature has no correlation with any other, which can result in better model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3b4ab3",
   "metadata": {},
   "source": [
    "```python\n",
    "# Train model without PCA\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(f\"Accuracy without PCA: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "\n",
    "# Train model with PCA-transformed data\n",
    "X_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "model_pca = RandomForestClassifier(random_state=42)\n",
    "model_pca.fit(X_train_pca, y_train)\n",
    "y_pred_pca = model_pca.predict(X_test_pca)\n",
    "print(f\"Accuracy with PCA (2 components): {accuracy_score(y_test, y_pred_pca):.4f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3aeb44",
   "metadata": {},
   "source": [
    "PCA reduces dimensionality but may lose some interpretability. Model performance without PCA is usually better because tree-based models don't need dimensionality reduction. PCA is more useful when features are highly correlated or when using distance-based models like KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac3169ae",
   "metadata": {},
   "source": [
    "### **Extra Reading**\n",
    "\n",
    "https://medium.com/@abhaysingh71711/linear-discriminant-analysis-lda-maximizing-class-separability-for-supervised-learning-f5f0a504c196"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6cc30ca",
   "metadata": {},
   "source": [
    "## **Linear Discriminant Analysis (LDA)**\n",
    "\n",
    "Like PCA, Linear discriminant analysis (LDA) can be used to transform data into new features that are linear combinations of the original ones. However, instead of maximizing the preserved variance from the original data, LDA works by maximizing the differences between known classes such that they are well separated in lower dimensional space. Linear discriminant analysis (LDA) is a dimensionality reduction technique used for classification problems with continuous independent variables. This technique can be used in a wide variety of applications including image recognition, marketing, biological classification, and more. It works by using linear algebra techniques to find a subspace within the space of independent variables that simplifies classification problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb1dc19",
   "metadata": {},
   "source": [
    "Let’s start with a simple example. Here’s a classification problem with two continuous independent variables and two classes: a red class and a blue class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdefbfaf",
   "metadata": {},
   "source": [
    "![image](images/lda_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3071d48b",
   "metadata": {},
   "source": [
    "In this example you could easily draw a line that would separate most of the red data points from the blue ones. But could you simplify the problem by reducing the data to a single dimension? If we project the data onto a 1-dimensional subspace, we might be able to separate red from blue with only one dimension instead of two. Here are two different ways of projecting the data onto 1-dimensional subspaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87eb5ea",
   "metadata": {},
   "source": [
    "![image](images/lda_2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdef0b8",
   "metadata": {},
   "source": [
    "![image](images/lda_3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11a473d0",
   "metadata": {},
   "source": [
    "In the first example, there’s no good way to separate red from blue. But in the second example you could easily choose a good decision boundary. This second subspace isn’t just any subspace, it’s actually the subspace that does the best job of separating red and blue. It was obtained by using LDA.\n",
    "\n",
    "This is what LDA does in general. It finds the best possible subspace for a given classification problem. When there are only two classes, LDA finds a subspace that maximizes the ratio of variance between the classes to variance within the classes. This means that it finds a subspace where classes are far apart from each other, but the observations within each individual class are close to each other. LDA can also be applied when there are more than two classes, but this is slightly more complicated.\n",
    "\n",
    "Once LDA finds the best subspace, data points can be projected onto that subspace. This yields a data set with fewer dimensions but nearly as much predictive power. LDA can also be used as a classifier by itself with the additional step of computing a decision boundary.\n",
    "\n",
    "Notice that LDA is similar to Principal Component Analysis (PCA). Both LDA and PCA are dimensionality reduction tools. They both reduce dimensions in a similar way: by using linear algebra to find optimal subspaces for a statistical or machine learning problem.\n",
    "\n",
    "A key difference between LDA and PCA is that they have different applications. PCA uses linear algebra techniques to find a subspace that maximizes the variance of a data set. This means that PCA basically finds the subspace that is best for linear regression problems. LDA, on the other hand, uses linear algebra techniques to find a subspace that is best for classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8f0dc3",
   "metadata": {},
   "source": [
    "```python\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "# Load data\n",
    "seeds = pd.read_csv('seeds.csv')\n",
    "X = seeds.drop('variety', axis=1)\n",
    "y = seeds['variety']\n",
    "\n",
    "# Create LDA model\n",
    "lda = LinearDiscriminantAnalysis(n_components=1)\n",
    "\n",
    "# Fit the data and create a subspace X_new\n",
    "X_new = lda.fit_transform(X, y)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51687350",
   "metadata": {},
   "source": [
    "X_new is a 1-dimensional subspace of X. We can use it to train a classifier like logistic regression.\n",
    "```python\n",
    "# Import library\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Create logistic regression model\n",
    "lr = LogisticRegression()\n",
    "\n",
    "# Fit the model\n",
    "lr.fit(X_new, y)\n",
    "\n",
    "# Model accuracy\n",
    "print(lr.score(X_new, y))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3507049a",
   "metadata": {},
   "source": [
    "\n",
    "## **T-distributed Stochastic Neighbor Embedding (t-SNE)**\n",
    "\n",
    "T-distributed stochastic neighbor embedding (t-SNE) is another dimensionality reduction technique, but it uses a nonlinear method to map each data point from a high-dimensional space to a 2- or 3-dimensional space. As its name implies, t-SNE uses the student t-distribution to model the points in a way such that similar points are mapped closer to each other and dissimilar points are further apart. As a result, t-SNE is great at preserving the local structure of the original data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
