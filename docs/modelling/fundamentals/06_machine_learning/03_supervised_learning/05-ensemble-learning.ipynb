{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26f188c7",
   "metadata": {},
   "source": [
    "# **Ensemble Learning**\n",
    "\n",
    "Why use one machine learning model to solve a problem when you can use many at the same time? Ensemble learning is the process of building a complex machine learning model by combining multiple base estimators as building blocks. The main goals for employing ensemble methods are to end up with a resultant machine learning model that is more performant and more robust than its components.\n",
    "\n",
    "In ensembling, often the base models that are chosen tend to underachieve on their own and are typically referred to as weak learners. The reason this is preferable is to have the model implementation be computationally efficient. Combining strong learners doesn’t necessarily make the resultant ensembled model more performant so one might as well choose learners that cost less computationally.\n",
    "\n",
    "What makes a model a “weak learner”? There are many ways in which a base model can be considered weak. For example, it can have high bias or high variance. The nature of the weakness of the base model is typically taken into consideration and is a design choice when determining the best ensembling method to use.\n",
    "\n",
    "The basic components of ensemble learning are:\n",
    "- Base models that are weak learners\n",
    "- An ensembling method that combines the base models to improve performance and robustness\n",
    "\n",
    "It is possible to have an ensemble model that performs worse than any one of its contributing base estimators. To circumvent this outcome it is important that the base estimators are uncorrelated and independent. Having a higher diversity among the trained base estimators leads to a stronger ensemble model.\n",
    "\n",
    "Three common ensembling methods in machine learning that will be covered briefly in this article and later on in separate modules are Bagging, Boosting, and Stacking.\n",
    "\n",
    "![image](images/bagging_vs_boosting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f62bc9",
   "metadata": {},
   "source": [
    "## **Bagging**\n",
    "\n",
    "As the name suggests, Bootstrap AGGregatING (also known as Bagging) is an ensemble method that combines the concepts of bootstrapping and aggregation. Bagging can be used for both classification and regression problems. Bagging methods use weak learners as base models that are complex and tend to suffer from high variance. Their weakness as models is due to the fact that they are built with only a subset of the available features and on a subset of the training data due to bootstrapping.\n",
    "\n",
    "If the full dataset is represented by the larger cookie in the figure above, each of the candies atop the cookie represents an individual training data instance. The smaller cookies in the Bagging panel represent a bootstrapped sample of the full training dataset. Bootstrapping refers to the method of sampling data with replacement.\n",
    "\n",
    "Bagging is a learning technique that is done in parallel. Each of the base models is trained independently of the others. Additionally, each base model is trained using only a subset of the original features. Even though the base models tend to be complex, they overfit to both a subset of the available training data and a subset of the available features. This allows them to be diverse from one another, often leading to a very strong ensemble model when aggregated. In the Bagging panel of the figure, the base models are decision trees that are relatively large and overfit to the bootstrapped subset of data provided to each of them.\n",
    "\n",
    "Once each of the base models is trained, the method for ensembling tends to be a simple aggregation technique over each of the models; a majority vote for classification problems and averaging for regression problems.\n",
    "\n",
    "A common implementation of a bagging algorithm that uses decision trees as their base model is the Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5745632",
   "metadata": {},
   "source": [
    "### **Random Forests**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fc3e60",
   "metadata": {},
   "source": [
    "We’ve seen that decision trees can be powerful supervised machine learning models. However, they’re not without their weaknesses — decision trees are often prone to overfitting. We’ve discussed some strategies to minimize this problem, like pruning, but sometimes that isn’t enough. We need to find another way to generalize our trees. This is where the concept of a random forest comes in handy.\n",
    "\n",
    "A random forest is an ensemble machine learning technique. A random forest contains many decision trees that all work together to classify new points. When a random forest is asked to classify a new point, the random forest gives that point to each of the decision trees. Each of those trees reports their classification and the random forest returns the most popular classification. It’s like every tree gets a vote, and the most popular classification wins.Some of the trees in the random forest may be overfit, but by making the prediction based on a large number of trees, overfitting will have less of an impact."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8ce3d3",
   "metadata": {},
   "source": [
    "You might be wondering how the trees in the random forest get created. After all, right now, our algorithm for creating a decision tree is deterministic — given a training set, the same tree will be made every time. To make a random forest, we use a technique called bagging, which is short for bootstrap aggregating"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7767f04c",
   "metadata": {},
   "source": [
    "#### **Random Samples**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65236ede",
   "metadata": {},
   "source": [
    "How it works is as follows: every time a decision tree is made, it is created using a different subset of the points in the training set. For example, if our training set had 1000 rows in it, we could make a decision tree by picking 100 of those rows at random to build the tree. This way, every tree is different, but all trees will still be created from a portion of the training data.\n",
    "\n",
    "In bootstrapping, we’re doing this process with replacement. Picture putting all 100 rows in a bag and reaching in and grabbing one row at random. After writing down what row we picked, we put that row back in our bag. This means that when we’re picking our 100 random rows, we could pick the same row more than once. In fact, it’s very unlikely, but all 100 randomly picked rows could all be the same row! Because we’re picking these rows with replacement, there’s no need to shrink our bagged training set from 1000 rows to 100. We can pick 1000 rows at random, and because we can get the same row more than once, we’ll still end up with a unique data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e4077c",
   "metadata": {},
   "source": [
    "Then after many trees have been made, the results are “aggregated” together. In the case of a classification task, often the aggregation is taking the majority vote of the individual classifiers. For regression tasks, often the aggregation is the average of the individual regressors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b555cbd8",
   "metadata": {},
   "source": [
    "#### **Random Feature Selection**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d92208",
   "metadata": {},
   "source": [
    "In addition to using bootstrapped samples of our dataset, we can continue to add variety to the ways our trees are created by randomly selecting the features that are used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e60e775",
   "metadata": {},
   "source": [
    "When we use a decision tree, all the features are used and the split is chosen as the one that increases the information gain the most. While it may seem counter-intuitive, selecting a random subset of features can help in the performance of an ensemble model. In the following example, we will use a random selection of features prior to model building to add additional variance to the individual trees. While an individual tree may perform worse, sometimes the increases in variance can help model performance of the ensemble model as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85812dad",
   "metadata": {},
   "source": [
    "The two steps we walked through above created trees on bootstrapped samples and randomly selecting features. These can be combined together and implemented at the same time! Combining them adds an additional variation to the base learners for the ensemble model. This in turn increases the ability of the model to generalize to new and unseen data, i.e., it minimizes bias and increases variance. Rather than re-doing this process manually, we will use `scikit-learn`‘s bagging implementation, `BaggingClassifier()`, to do so."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b387c2b",
   "metadata": {},
   "source": [
    "#### **sklearn.ensemble.BaggingClassifier**\n",
    "\n",
    "Much like other models we have used in scikit-learn, we instantiate a instance of `BaggingClassifier()` and specify the parameters. The first parameter, `base_estimator` refers to the machine learning  model that is being bagged. In the case of random forests, the base estimator would be a decision tree. We are going to use a decision tree classifier WITH a `max_depth` of 5, this will be instantiated with `BaggingClassifier(DecisionTreeClassifier(max_depth=5))`.\n",
    "\n",
    "After the model has been defined, methods `.fit()`, `.predict()`, `.score()` can be used as expected. Additional hyperparameters specific to bagging include the number of estimators (`n_estimators`) we want to use and the maximum number of features we’d like to keep (`max_features`).\n",
    "\n",
    "Note: While we have focused on decision tree classifiers (as this is the base learner for a random forest classifier), this procedure of bagging is not specific to decision trees, and in fact can be used for any base classifier or regression model. The scikit-learn implementation is generalizable and can be used for other base models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bce667db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of Bagged Classifier, 10 estimators:\n",
      "0.8912037037037037\n",
      "Accuracy score of Bagged Classifier, 10 estimators, 10 max features:\n",
      "0.9074074074074074\n",
      "Accuracy score of Logistic Regression, 10 estimators:\n",
      "0.875\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "df = pd.read_csv(\n",
    "    \"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\",\n",
    "    names=[\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\", \"accep\"],\n",
    ")\n",
    "df[\"accep\"] = ~(df[\"accep\"] == \"unacc\")  # 1 is acceptable, 0 if not acceptable\n",
    "X = pd.get_dummies(df.iloc[:, 0:6], drop_first=True)\n",
    "y = df[\"accep\"]\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, y, random_state=0, test_size=0.25)\n",
    "\n",
    "# 1. Bagging classifier with 10 Decision Tree base estimators\n",
    "bag_dt = BaggingClassifier(estimator=DecisionTreeClassifier(max_depth=5), n_estimators=10)\n",
    "bag_dt.fit(x_train, y_train)\n",
    "\n",
    "print(\"Accuracy score of Bagged Classifier, 10 estimators:\")\n",
    "bag_accuracy = bag_dt.score(x_test, y_test)\n",
    "print(bag_accuracy)\n",
    "\n",
    "# 2.Set `max_features` to 10.\n",
    "bag_dt_10 = BaggingClassifier(\n",
    "    estimator=DecisionTreeClassifier(max_depth=5), n_estimators=10, max_features=10\n",
    ")\n",
    "bag_dt_10.fit(x_train, y_train)\n",
    "\n",
    "print(\"Accuracy score of Bagged Classifier, 10 estimators, 10 max features:\")\n",
    "bag_accuracy_10 = bag_dt_10.score(x_test, y_test)\n",
    "print(bag_accuracy_10)\n",
    "\n",
    "# 3. Change base estimator to Logistic Regression\n",
    "\n",
    "bag_lr = BaggingClassifier(estimator=LogisticRegression(), n_estimators=10, max_features=10)\n",
    "bag_lr.fit(x_train, y_train)\n",
    "\n",
    "print(\"Accuracy score of Logistic Regression, 10 estimators:\")\n",
    "bag_accuracy_lr = bag_lr.score(x_test, y_test)\n",
    "print(bag_accuracy_lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94fd094f",
   "metadata": {},
   "source": [
    "#### **sklearn.ensemble.RandomForestClassifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36d5b39",
   "metadata": {},
   "source": [
    "The random forest algorithm has a slightly different way of randomly choosing features. Rather than choosing a single random set at the onset, each split chooses a different random set.\n",
    "\n",
    "For example, when finding which feature to split the data on the first time, we might randomly choose to only consider the price of the car, the number of doors, and the safety rating. After splitting the data on the best feature from that subset, we’ll likely want to split again. For this next split, we’ll randomly select three features again to consider. This time those features might be the cost of maintenance, the number of doors, and the size of the trunk. We’ll continue this process until the tree is complete.\n",
    "\n",
    "One question to consider is how to choose the number of features to randomly select. Why did we choose 3 in this example? A good rule of thumb is select as many features as the square root of the total number of features. Our car dataset doesn’t have a lot of features, so in this example, it’s difficult to follow this rule. But if we had a dataset with 25 features, we’d want to randomly select 5 features to consider at every split point.\n",
    "\n",
    "You now have the ability to make a random forest using your own decision trees. However, scikit-learn has a `RandomForestClassifier()` class that will do all of this work for you! `RandomForestClassifier` is in the `sklearn.ensemble` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f904994f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest parameters:\n",
      "{'bootstrap': True, 'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': None, 'max_features': 'sqrt', 'max_leaf_nodes': None, 'max_samples': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'n_estimators': 100, 'n_jobs': None, 'oob_score': False, 'random_state': None, 'verbose': 0, 'warm_start': False}\n",
      "Test set accuracy:\n",
      "0.9537037037037037\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score\n",
    "\n",
    "# 1. Create a Random Forest Classifier and print its parameters\n",
    "rf = RandomForestClassifier()\n",
    "print(\"Random Forest parameters:\")\n",
    "rf_params = rf.get_params()\n",
    "print(rf_params)\n",
    "\n",
    "# 2. Fit the Random Forest Classifier to training data and calculate accuracy score on the test data\n",
    "rf.fit(x_train, y_train)\n",
    "y_pred = rf.predict(x_test)\n",
    "print(\"Test set accuracy:\")\n",
    "rf_accuracy = rf.score(x_test, y_test)\n",
    "print(rf_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2382ce0b",
   "metadata": {},
   "source": [
    "#### **sklearn.ensemble.RandomForestRegressor**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e1ef6f",
   "metadata": {},
   "source": [
    "Just like in decision trees, we can use random forests for regression as well! It is important to know when to use regression or classification — this usually comes down to what type of variable your target is. Now, instead of a classification task, we will use `scikit-learn`‘s `RandomForestRegressor()` to carry out a regression task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d96a419",
   "metadata": {},
   "source": [
    "### **Extra Reading**\n",
    "\n",
    "https://medium.com/data-science/random-forest-explained-a-visual-guide-with-code-examples-9f736a6e1b3c\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d252ce",
   "metadata": {},
   "source": [
    "## **Boosting**\n",
    "\n",
    "Boosting is an ensemble learning technique where the weak learners are too simple and tend to suffer from high bias. In the Boosting panel of the figure above, the base models are decision trees with only one level, a decision stump. Decision stumps can only make a decision based off of one feature at a time, causing them to underfit the data substantially.\n",
    "\n",
    "Boosting is a sequential learning technique where each of the base models builds off the previous model. Each subsequent model aims to improve the performance of the final ensembled model by attempting to fix the errors in the previous stage.\n",
    "\n",
    "In the Boosting panel of the figure, you may notice that some of the candies atop the cookies are larger than the others. These particular training instances were misclassified by the previous decision stump and are therefore given more weight by the next decision stump. This is one method in which boosting methods may learn from their mistakes.\n",
    "\n",
    "There are two important decisions that need to be made to perform boosted ensembling:\n",
    "- Sequential Fitting Method\n",
    "- Aggregation Method\n",
    "\n",
    "Two common implementations of the boosting algorithm are Adaptive Boosting and Gradient Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25720503",
   "metadata": {},
   "source": [
    "### **Adaptive Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "581904cf",
   "metadata": {},
   "source": [
    "Adaptive Boosting (or AdaBoost) is a sequential ensembling method that can be used for both classification and regression. It can use any base machine learning model, though it is most commonly used with decision trees.\n",
    "\n",
    "For AdaBoost, the Sequential Fitting Method is accomplished by updating the weight attached to each of the training dataset observations as we proceed from one base model to the next. The Aggregation Method is a weighted sum of those base models where the model weight is dependent on the error of that particular estimator.\n",
    "\n",
    "The training of an AdaBoost model is the process of determining the training dataset observation weights at each step as well as the final weight for each base model for aggregation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8d9ec1",
   "metadata": {},
   "source": [
    "![image](images/adaptive_boosting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8aca678",
   "metadata": {},
   "source": [
    " We already said that the base models for boosting are supposed to be very simple and tend to underfit. That is correct, and for this reason we use the simplest version of a decision tree, known as a decision stump. A decision stump only makes a single decision, so the resultant estimator only has two leaf nodes.\n",
    "\n",
    "Taking a look at the Result of the 1st Base Model, we see that the decision boundary, that is the border between the lighter green and lighter red regions, does a decent job of separating the green circles from the red triangles. However we do notice that there are two red triangles in the light green region. This indicates that they have been classified incorrectly by the decision stump.\n",
    "\n",
    "Each of the base models will contribute a different amount to the final ensemble model. The influence that a particular base model contributes is going to be dependent on the number of errors it makes, or for regression, the magnitude of the errors it makes. We do not want a decision stump that does a terrible job of classifying the data to have the same say as a decision stump that does a great job. Once we are able to evaluate the Result of the 1st Base Model, we can Weight the Model and assign it a value, here indicated by alpha_1.\n",
    "\n",
    "To prepare for the next stage of the sequential learning process, we need to Reweight the Data. The instances of the training data that were classified incorrectly by the 1st Base Model, the two red triangles in the middle right, are given a larger weight than the other data instances indicated by their larger size. By assigning those misclassified points a larger weight, we are asking the the 2nd Base Model to give them preferential treatment during the Model Fitting.\n",
    "\n",
    "Taking a look at the Result of the 2nd Base Model, we see that is exactly what happens. The two larger red triangles are classified correctly by the 2nd Base Model. Once again we assign the base model a weight, alpha_2 proportional to the errors it makes and prepare for the next stage of the sequential learning by reweighting the training data. The instances that were incorrectly classified by the 2nd Base Model, the two green circles in on the center right, are given a larger weight.\n",
    "\n",
    "Once we have reached the predefined number of estimators for our AdaBoost model, the base models are ready to aggregate. In this example we have chosen n_estimators = 3. The influence of each base model in the final ensemble model will be proportional to the alpha it was assigned during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5fd04d",
   "metadata": {},
   "source": [
    "#### **sklearn.ensemble.AdaBoostClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93bf558b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ccp_alpha': 0.0, 'class_weight': None, 'criterion': 'gini', 'max_depth': 1, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'monotonic_cst': None, 'random_state': None, 'splitter': 'best'}\n",
      "{'algorithm': 'deprecated', 'estimator__ccp_alpha': 0.0, 'estimator__class_weight': None, 'estimator__criterion': 'gini', 'estimator__max_depth': 1, 'estimator__max_features': None, 'estimator__max_leaf_nodes': None, 'estimator__min_impurity_decrease': 0.0, 'estimator__min_samples_leaf': 1, 'estimator__min_samples_split': 2, 'estimator__min_weight_fraction_leaf': 0.0, 'estimator__monotonic_cst': None, 'estimator__random_state': None, 'estimator__splitter': 'best', 'estimator': DecisionTreeClassifier(max_depth=1), 'learning_rate': 1.0, 'n_estimators': 5, 'random_state': None}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Load dataset to a pandas DataFrame\n",
    "path_to_data = \"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\"\n",
    "column_names = [\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\", \"accep\"]\n",
    "df = pd.read_csv(path_to_data, names=column_names)\n",
    "\n",
    "target_column = \"accep\"\n",
    "raw_feature_columns = [col for col in column_names if col != target_column]\n",
    "\n",
    "# Create dummy variables from the feature columns\n",
    "X = pd.get_dummies(df[raw_feature_columns], drop_first=True)\n",
    "\n",
    "# Convert target column to binary variable; 0 if 'unacc', 1 otherwise\n",
    "df[target_column] = np.where(df[target_column] == \"unacc\", 0, 1)\n",
    "y = df[target_column]\n",
    "\n",
    "# Split the full dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=0.3)\n",
    "\n",
    "\n",
    "# 1. Create a decision stump base model using the Decision Tree Classifier and print its parameters\n",
    "decision_stump = DecisionTreeClassifier(max_depth=1)\n",
    "print(decision_stump.get_params())\n",
    "\n",
    "# 2. Create an Adaptive Boost Classifier and print its parameters\n",
    "ada_classifier = AdaBoostClassifier(estimator=decision_stump, n_estimators=5)\n",
    "print(ada_classifier.get_params())\n",
    "\n",
    "# 3. Fit the Adaptive Boost Classifier to the training data and get the list of predictions\n",
    "ada_classifier.fit(X_train, y_train)\n",
    "y_pred = ada_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be25bd4",
   "metadata": {},
   "source": [
    "### **Gradient Boosting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eff38e0",
   "metadata": {},
   "source": [
    "Gradient Boosting is a sequential ensembling method that can be used for both classification and regression. It can use any base machine learning model, though it is most commonly used with decision trees, known as Gradient Boosted Trees.\n",
    "\n",
    "For Gradient Boost, the Sequential Fitting Method is accomplished by fitting a base model to the negative gradient of the error in the previous stage. The Aggregation Method is a weighted sum of those base models where the model weight is constant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a6bcc46",
   "metadata": {},
   "source": [
    "![image](images/gradient_boosting.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc1ecd3",
   "metadata": {},
   "source": [
    "The errors will be greater for the training data instances where the model did not do as good of a job with its prediction and will be lower on training data instances where the model fit the data well.\n",
    "\n",
    "In the next stage of the sequential learning process, we fit the 2nd Base Model. Here is where the interesting part comes in. Instead of fitting the model to the target values y_actual as we are typically used to doing in machine learning, we actually fit the model on the errors of the previous stage, in this case $h_1$. The 2nd Base Model is literally learning from the mistakes of the 1st Base Model through those residuals that were calculated.\n",
    "\n",
    "The results of the 2nd Base Model are multiplied by a constant learning rate, alpha, and added to the results of the 1st Base Model to give the set of updated predictions, The results of the second base model, which was tasked with fitting the errors of the first base model are multiplied by a constant learning rate, alpha and added to the results of the first base model to give us a set of updated predictions, $y_2$.\n",
    "\n",
    "The subsequent stages repeat the same steps. At stage N, the base model is fit on the errors calculated at the previous stage $h_{(N-1)}$. The new model that is fit is multiplied by the constant learning rate alpha and added to the predictions of the previous stage.\n",
    "\n",
    "Once we have reached the predefined number of estimators for our Gradient Boosting model or the residual errors are not changing between iterations, the model will stop training and we end up with the resultant ensemble model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc99d85",
   "metadata": {},
   "source": [
    "#### **sklearn.ensemble.GradientBoostingClassifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e86f4f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ccp_alpha': 0.0, 'criterion': 'friedman_mse', 'init': None, 'learning_rate': 0.1, 'loss': 'log_loss', 'max_depth': 3, 'max_features': None, 'max_leaf_nodes': None, 'min_impurity_decrease': 0.0, 'min_samples_leaf': 1, 'min_samples_split': 2, 'min_weight_fraction_leaf': 0.0, 'n_estimators': 15, 'n_iter_no_change': None, 'random_state': None, 'subsample': 1.0, 'tol': 0.0001, 'validation_fraction': 0.1, 'verbose': 0, 'warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset to a pandas DataFrame\n",
    "path_to_data = \"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\"\n",
    "column_names = [\"buying\", \"maint\", \"doors\", \"persons\", \"lug_boot\", \"safety\", \"accep\"]\n",
    "\n",
    "df = pd.read_csv(path_to_data, names=column_names)\n",
    "target_column = \"accep\"\n",
    "raw_feature_columns = [col for col in column_names if col != target_column]\n",
    "\n",
    "# Create dummy variables from the feature columns\n",
    "X = pd.get_dummies(df[raw_feature_columns], drop_first=True)\n",
    "\n",
    "# Convert target column to binary variable; 0 if 'unacc', 1 otherwise\n",
    "df[target_column] = np.where(df[target_column] == \"unacc\", 0, 1)\n",
    "y = df[target_column]\n",
    "\n",
    "# Split the full dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=123, test_size=0.3)\n",
    "\n",
    "# 1. Create a Gradient Boosting Classifier and print its parameters\n",
    "grad_classifier = GradientBoostingClassifier(n_estimators=15)\n",
    "\n",
    "print(grad_classifier.get_params())\n",
    "\n",
    "# 2. Fit the Gradient Boosted Trees Classifier to the training data and get the list of predictions\n",
    "grad_classifier.fit(X_train, y_train)\n",
    "y_pred = grad_classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551890a5",
   "metadata": {},
   "source": [
    "### **Extreme Gradient Boosting (XGBoost)**\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is an optimized version of Gradient Boosting that is faster and more efficient than traditional gradient boosting algorithms. It uses second-order derivatives to approximate loss functions, improving optimization. Key Features\n",
    "- Regularization (L1 & L2) to prevent overfitting.\n",
    "- Tree Pruning to prevent unnecessary splits.\n",
    "- Parallel & Distributed Computing for scalability.\n",
    "- Handling Missing Values Automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdb6f001",
   "metadata": {},
   "source": [
    "#### **xgboost.xgb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5e135523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost MSE: 0.0432\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb\n",
    "\n",
    "# Load dataset\n",
    "data = datasets.load_breast_cancer()  # For regression example\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define XGBoost model\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    objective=\"reg:squarederror\", n_estimators=100, learning_rate=0.1, max_depth=4\n",
    ")\n",
    "\n",
    "# Train model\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"XGBoost MSE: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7de561",
   "metadata": {},
   "source": [
    "#### **Extra Reading**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047ada65",
   "metadata": {},
   "source": [
    "https://medium.com/sfu-cspmp/xgboost-a-deep-dive-into-boosting-f06c9c41349\n",
    "\n",
    "https://medium.com/@prathameshsonawane/xgboost-how-does-this-work-e1cae7c5b6cb\n",
    "\n",
    "https://towardsdatascience.com/xgboost-the-definitive-guide-part-1-cc24d2dcd87a/\n",
    "\n",
    "https://python.plainenglish.io/mastering-xgboost-a-beginners-guide-to-boosting-classification-performance-8a6de637a293\n",
    "\n",
    "https://python.plainenglish.io/anomaly-detection-end-to-end-real-life-bank-card-fraud-detection-with-xgboost-2a343f761fa9\n",
    "\n",
    "https://medium.com/data-science/visualizing-xgboost-parameters-a-data-scientists-guide-to-better-models-38757486b813"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d926d7c",
   "metadata": {},
   "source": [
    "### **LightGBM**\n",
    "\n",
    "LightGBM is a gradient boosting framework optimized for speed and efficiency. Instead of growing trees level-wise (like XGBoost), it grows trees leaf-wise, which allows for faster training and better accuracy. Key features include:\n",
    "- Histogram-based learning (reduces computation time).\n",
    "- Leaf-wise tree growth (improves accuracy but may overfit).\n",
    "- Handles large datasets efficiently.\n",
    "- Built-in categorical feature handling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b60483",
   "metadata": {},
   "source": [
    "#### **lightgbm.lgb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a596b7cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000081 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 595\n",
      "[LightGBM] [Info] Number of data points in the train set: 353, number of used features: 10\n",
      "[LightGBM] [Info] Start training from score 153.736544\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "LightGBM MSE: 3203.0861\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.datasets import load_diabetes  # Regression example\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "data = load_diabetes()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create dataset for LightGBM\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "# Define parameters\n",
    "params = {\"objective\": \"regression\", \"metric\": \"mse\", \"learning_rate\": 0.1, \"num_leaves\": 31}\n",
    "\n",
    "# Train model\n",
    "lgb_model = lgb.train(params, train_data, num_boost_round=100, valid_sets=[test_data])\n",
    "\n",
    "# Predict\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"LightGBM MSE: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8ceefb",
   "metadata": {},
   "source": [
    "#### **Extra Reading**\n",
    "\n",
    "https://medium.com/@turkishtechnology/light-gbm-light-and-powerful-gradient-boost-algorithm-eaa1e804eca8\n",
    "\n",
    "https://medium.com/@mohtasim.hossain2000/mastering-lightgbm-an-in-depth-guide-to-efficient-gradient-boosting-8bfeff15ee17\n",
    "\n",
    "https://towardsdatascience.com/a-quick-guide-to-lightgbm-library-ef5385db8d10/\n",
    "\n",
    "https://towardsdatascience.com/lightgbm-the-fastest-option-of-gradient-boosting-1fb0c40948a3/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b576b86d",
   "metadata": {},
   "source": [
    "### **CatBoost**\n",
    "\n",
    "CatBoost is designed specifically for categorical data and automatically handles categorical variables without requiring one-hot encoding. However, CatBoost introduces several key innovations that distinguish it from other gradient boosting methods:\n",
    "\n",
    "1. Handling Categorical Features: CatBoost converts categorical values into numerical values using a novel algorithm that takes into account the target variable for ordering categorical levels. This process, known as “target statistics,” helps in reducing overfitting and provides a more accurate representation of categorical data.\n",
    "\n",
    "2. Ordered Boosting: One of the core innovations of CatBoost is its ordered boosting mechanism. Traditional gradient boosting methods can suffer from prediction shift due to the overlap between the training data for the base models and the data used to calculate the gradients. CatBoost addresses this by introducing a random permutation of the dataset in each iteration and using only the data before each example in the permutation for training. This approach reduces overfitting and improves model robustness.\n",
    "\n",
    "3. Symmetric Trees: CatBoost builds balanced trees, also known as symmetric trees, as its base predictors. Unlike traditional gradient boosting methods that build trees leaf-wise or depth-wise, CatBoost’s symmetric trees ensure that all leaf nodes at the same level share the same decision rule. This leads to faster execution and reduces the likelihood of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab14c853",
   "metadata": {},
   "source": [
    "#### **catboost.CatBoostClassifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd5d8ea7",
   "metadata": {},
   "source": [
    "```python\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load dataset\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize CatBoostClassifier\n",
    "model = CatBoostClassifier(iterations=1000, learning_rate=0.1, depth=3, verbose=200)\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7b78ff",
   "metadata": {},
   "source": [
    "- `iterations (num_boost_round)`: The number of gradient-boosted trees to construct. A higher value generally leads to better performance but increases training time and risks overfitting.\n",
    "- `learning_rate`: The step size used in each gradient boosting iteration. Lower learning rates slow down learning for a more gradual approach (possibly preventing overfitting), while higher values converge faster (but risks overshooting the optimal solution).\n",
    "- `depth (max_depth)`: The maximum depth of each decision tree. Deeper trees allow for more complex interactions, but increase overfitting risk. Experiment with different depths to strike a balance.\n",
    "- `l2_leaf_reg (reg_lambda)`: L2 regularization coefficient applied to leaf values. Larger values impose heavier regularization to prevent overfitting, potentially at the cost of some accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca95bbdc",
   "metadata": {},
   "source": [
    "#### **catboost.CatBoostRegressor**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b565be0",
   "metadata": {},
   "source": [
    "```python\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Initialize CatBoostRegressor\n",
    "model = CatBoostRegressor(iterations=1000, learning_rate=0.1, depth=3, verbose=200)\n",
    "\n",
    "# Fit model\n",
    "model.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbd0205",
   "metadata": {},
   "source": [
    "#### **Extra Reading**\n",
    "\n",
    "https://medium.com/we-talk-data/what-is-catboost-a-guide-to-boosting-techniques-f370a41f989d\n",
    "\n",
    "https://ai.plainenglish.io/understand-catboost-intuition-and-training-process-e0bf258065f2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274e1296",
   "metadata": {},
   "source": [
    "## **Stacking (Meta-Ensembling)**\n",
    "Stacking is an extremely flexible ensembling technique where a final model is trained to learn how to best combine a set of base models to make strong predictions. In contrast to bagging and boosting, the base models in stacking do not need to be the same type of learning algorithm.\n",
    "\n",
    "While bagging and boosting are built with base models that are weak learners, that does not necessarily have to be the case for stacking. A stacking algorithm can be used to combine decently performing learners as well. Unlike bagging, there are no subsampling processes used. The stacking model effectively uses a full training set.\n",
    "\n",
    "Consider the scenario in which we are handling a classification problem where we are exploring different models. When using a decision tree we find that our model has poor generalization performance as a result of overfitting. Additionally, when using a logistic regression classification model, we discover our model parameters are hard to tune due to highly-correlated features. We might discover that while each model has a unique advantage over the others, each may also have a distinct drawback such as poor generalization accuracy or the inability to predict a specific class. As it turns out, there’s no rule stating we can’t take the best of both worlds!\n",
    "\n",
    "Stacking can be thought of as a democracy of machine learning models, where different models are trained and subsequently cast their vote through their predictions. A majority-rules approach can be used for determining the final model prediction if we weighted each estimator equally. In practice, each base estimator may need to be weighted differently, so we have a later-stage model to learn how to appropriately weigh the predictions of all the prior base estimators."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe0cb30",
   "metadata": {},
   "source": [
    "### **Training Base Estimators**\n",
    "\n",
    "We can select from combinations of different base estimators, such as a logistic regression model in combination with a decision tree. We could additionally select models of the same learning algorithms, but with different parameters, such as multiple decision trees with varying depths. The number of estimators is arbitrary, so it’s good practice to explore how different combinations behave.\n",
    "\n",
    "This introduces a problem, however. The estimators would be making predictions on data used in training. This puts our model at risk of overfitting. To avoid this, we use K-Fold Cross Validation as described next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acedadb5",
   "metadata": {},
   "source": [
    "![image](images/stacking.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8a59a1",
   "metadata": {},
   "source": [
    "To assemble our ensemble, we’ll make a dictionary of base estimators. This will be contained within the `level_0_estimators` dict. Also, our final estimator will be a Random Forest as represented with `level_1_estimator`. Notice also how we prepare to add new features to our training dataset (as columns) in `level_0_columns`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "882f0902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "rand_state = 42\n",
    "\n",
    "level_0_estimators = dict()\n",
    "level_0_estimators[\"logreg\"] = LogisticRegression(random_state=rand_state)\n",
    "level_0_estimators[\"forest\"] = RandomForestClassifier(random_state=rand_state)\n",
    "\n",
    "level_0_columns = [f\"{name}_prediction\" for name in level_0_estimators.keys()]\n",
    "\n",
    "level_1_estimator = RandomForestClassifier(random_state=rand_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c93fb80a",
   "metadata": {},
   "source": [
    "### **K-Fold Cross Validation**\n",
    "\n",
    "Consider 10 segments (or folds) and a stacking model that uses a logistic regression model and a decision tree model. Each estimator can be trained using data from 9 of the segments, and make predictions on the excluded 10th segment. We then append the predictions as new features to that 10th segment. Now 1/10th of the training data has two new features: one is the prediction made by the logistic regression model and the other is the prediction made by the decision tree model.\n",
    "\n",
    "We want to do the same with the other 9 segments, so we rotate the excluded segment and repeat this process until all training data points are augmented with new features. The end result is a prediction made on each training sample, without having seen the sample during the training process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3be67d",
   "metadata": {},
   "source": [
    "Handling our k-fold cross-validation is fairly straightforward using `sklearn.model_selection.StratifiedKFold` from the scikit-learn library. The kfold is then given to the instantiated `StackingClassifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe6603c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=rand_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f72332b9",
   "metadata": {},
   "source": [
    "\n",
    "### **Feature Augmentation**\n",
    "\n",
    "In our stacking setup, the base estimators need to be trained to make predictions on our training data. The prediction of each estimator will be appended to the corresponding data sample as a new feature. We thus augment the training data set with this additional information. The augmented training set is used by our later-stage stacking model to make the final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8151c2",
   "metadata": {},
   "source": [
    "![image](images/feature_augmentation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d679e166",
   "metadata": {},
   "source": [
    "So, in summary, say our training dataset has 10,000 samples, 10 features, and we select 3 base estimators. We would train each base estimator on the training set and make predictions on the training set. Each estimator would make a prediction on each sample, therefore each sample will have 3 predictions. These 3 predictions are appended to the pre-existing 10 features. This leaves us with 10,000 training samples with 13 features each."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d31ef6b",
   "metadata": {},
   "source": [
    "\n",
    "| Sample | Logistic Regression prediction | Decision Tree prediction | pH | Conductivity | Turbidity | Potability |\n",
    "| :----- | :----------------------------- | :----------------------- | :------- | :------------ | :-------- | :---------- |\n",
    "| 1 | 0 | 1 | 8.316766 | 363.266516 | 4.628771 | 1 |\n",
    "| 2 | 0 | 0 | 9.092223 | 398.410813 | 4.075075 | 0 |\n",
    "| 3 | 1 | 1 | 5.584087 | 280.467916 | 2.559708 | 1 |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b194fedd",
   "metadata": {},
   "source": [
    "### **sklearn.ensemble.StackingClassifier**\n",
    "With our augmented training set, we’re ready to prepare the final model that will make the official prediction from our stacking setup.\n",
    "\n",
    "This later-stage model is a learning algorithm that we select just like the base estimators. We may even reuse an algorithm from an earlier stage here. The purpose of this model is to learn the proper weighting of the earlier estimator given our training samples now include a data point from each estimator. Some estimators may perform better than others, so our overall model should account for this.\n",
    "\n",
    "The only difference in the training process is that the model will be designed to accept samples of the augmented size rather than the given size. This means if the given data set has n features and m base estimators, this model will require n + m features on each sample.\n",
    "\n",
    "And that’s it! Once the later-stage model is trained, we feed testing data samples into the base estimators, which will append their predictions to the data sample. This sample is then given to the later-stage model for our final prediction.\n",
    "\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "\n",
    "\n",
    "stacking_clf = StackingClassifier(\n",
    "    estimators=list(level_0_estimators.items()),\n",
    "    final_estimator=level_1_estimator,\n",
    "    passthrough=True,\n",
    "    cv=kfold,\n",
    "    stack_method=\"predict_proba\",\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    stacking_clf.fit_transform(X_train, y_train), columns=level_0_columns + list(X_train.columns)\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6222b594",
   "metadata": {},
   "source": [
    "Calling `fit_transform` on our classifier manages a lot of the heavy lifting. It will handle the training of our level_0 base estimators along with the level_1_estimator, make cross-validated predictions on the training set, and augment the training set with predictions from each estimator. Let’s see how the resulting training dataset looks:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e72b8a6",
   "metadata": {},
   "source": [
    "\n",
    "| Sample | logreg\\_prediction | forest\\_prediction | pH | Hardness | Solids | Chloramines | Sulfate | Conductivity | Organic\\_carbon | Trihalomethanes | Turbidity |\n",
    "| :----- | :----------------- | :----------------- | :------- | :------- | :---------- | :---------- | :---------- | :------------ | :------------- | :--------------- | :-------- |\n",
    "| 0 | 0.417408 | 0.17 | 9.927024 | 208.490738 | 19666.992792 | 8.008618 | 340.237824 | 482.842435 | 11.360427 | 85.829113 | 4.051733 |\n",
    "| 1 | 0.379019 | 0.26 | 8.769676 | 215.368742 | 13969.438863 | 7.548543 | 322.799070 | 369.016667 | 18.919188 | 54.755214 | 3.776718 |\n",
    "| 2 | 0.409582 | 0.87 | 8.077261 | 125.302719 | 23931.282833 | 8.773162 | 317.693331 | 398.328789 | 15.279583 | 62.668356 | 4.279871 |\n",
    "| 3 | 0.407893 | 0.73 | 9.739562 | 166.948864 | 13623.160063 | 7.235922 | 385.059134 | 369.591289 | 12.322604 | 68.505852 | 2.568080 |\n",
    "| 4 | 0.399452 | 0.04 | 5.343075 | 211.662091 | 45166.912141 | 6.651801 | 279.767500 | 485.959717 | 19.682337 | 70.546862 | 4.240032 |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a873d05",
   "metadata": {},
   "source": [
    "Finally, with our full model trained, we can make predictions. Let’s compare how our Stacking classifier performed with how a lone linear model and a lone decision tree model perform!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1caf11",
   "metadata": {},
   "source": [
    "```python\n",
    "y_val_pred = stacking_clf.predict(X_test)\n",
    "stacking_accuracy = accuracy_score(y_test, y_val_pred)\n",
    "\n",
    "vanilla_logistic_regression = LogisticRegression(random_state=rand_state).fit(X_train, y_train)\n",
    "lr_accuracy = accuracy_score(y_test, vanilla_logistic_regression.predict(X_test))\n",
    "                                   \n",
    "vanilla_decision_tree = RandomForestClassifier(random_state=rand_state).fit(X_train, y_train)\n",
    "dt_accuracy =  accuracy_score(y_test, vanilla_decision_tree.predict(X_test))\n",
    "\n",
    "print(f'Stacking accuracy: {stacking_accuracy:.4f}')\n",
    "print(f'Logistic Regression accuracy: {lr_accuracy:.4f}')\n",
    "print(f'Decision Tree accuracy: {dt_accuracy:.4f}')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f0e6f1",
   "metadata": {},
   "source": [
    "### **Limitations**\n",
    "\n",
    "Stacking is very powerful in that we remove the occasionally difficult choice of which learning algorithm to use for our problem. Depending on the use case, this benefit does come with some limitations worth noting:\n",
    "- Because we have an arbitrary number of learning algorithms in use, training an entire stacking model is computationally expensive. This is also true for deployed inference models.\n",
    "- Such a large model with many parameters means that a plethora of data is needed for proper training. Small datasets won’t see significant gains with stacking. Stacking models typically yields marginal gains over the best single estimator used for the same problem. When successful, a stacked model may reduce error by 2% or less.\n",
    "\n",
    "Stacking offers some creativity and openendedness in how we want to build our model. A vanilla stacking model may have one tier of models to contribute to the final prediction. We could alternatively construct a multi-tier approach in which one level of models feeds into a later stage of models before making a final prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9645f89f",
   "metadata": {},
   "source": [
    "# **Extra Reading**\n",
    "\n",
    "https://medium.com/@hassaanidrees7/gradient-boosting-vs-random-forest-which-ensemble-method-should-you-use-9f2ee294d9c6\n",
    "\n",
    "https://blog.gopenai.com/how-bayesian-ml-model-tuning-avoids-overfitting-hyperoptimized-gradient-boosting-hgboost-in-9ef68f719b07\n",
    "\n",
    "https://pub.towardsai.net/bagging-vs-boosting-the-power-of-ensemble-methods-in-machine-learning-6404e33524e6\n",
    "\n",
    "https://pub.towardsai.net/reinforcement-learning-enhanced-gradient-boosting-machines-77457e8cb4d9\n",
    "\n",
    "https://medium.com/code-applied/time-series-secrets-prediction-model-with-lightgbm-572a689e974f\n",
    "\n",
    "https://medium.com/@vikashsinghy2k/xgboost-for-regression-in-python-build-train-and-evaluate-your-model-19eec4ac2f74\n",
    "\n",
    "https://medium.com/chat-gpt-now-writes-all-my-articles/a-new-gradient-boosting-method-from-standford-research-ngboost-python-code-included-875004ef3f47\n",
    "\n",
    "https://medium.com/@kylejones_47003/boosting-stacking-and-bagging-for-ensemble-models-for-time-series-analysis-with-python-d74ab9026782\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
