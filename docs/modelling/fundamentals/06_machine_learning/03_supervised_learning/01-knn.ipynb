{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb81c6d0",
   "metadata": {},
   "source": [
    "# **K Nearest Neighbors**\n",
    "\n",
    "K-Nearest Neighbors (KNN) is a classification algorithm. The central idea is that data points with similar attributes tend to fall into similar categories. If you have a dataset of points where the class of each point is known, you can take a new point with an unknown class, find it’s nearest neighbors, and classify it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558ffd3a",
   "metadata": {},
   "source": [
    "## **First Principles**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7229bff1",
   "metadata": {},
   "source": [
    "### **Distance Formula**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddcb9a4b",
   "metadata": {},
   "source": [
    "#### **Distance Between Points - 2D**\n",
    "\n",
    "We need to define what it means for two points to be close together or far apart. To do this, we’re going to use the Distance Formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6c9603e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.770329614269007\n",
      "38.897300677553446\n"
     ]
    }
   ],
   "source": [
    "star_wars = [125, 1977]\n",
    "raiders = [115, 1981]\n",
    "mean_girls = [97, 2004]\n",
    "\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "    length_difference = (movie1[0] - movie2[0]) ** 2\n",
    "    year_difference = (movie1[1] - movie2[1]) ** 2\n",
    "    distance = (length_difference + year_difference) ** 0.5\n",
    "    return distance\n",
    "\n",
    "\n",
    "print(distance(star_wars, raiders))\n",
    "print(distance(star_wars, mean_girls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bdea87",
   "metadata": {},
   "source": [
    "#### **Distance Between Points - nth Dimensional**\n",
    "\n",
    "The generalized distance formula between points A and B is as follows:\n",
    "\n",
    "$\\sqrt{(A_1-B_1)^2+(A_2-B_2)^2+...+(A_n-B_n)^2}$\n",
    "\n",
    "Here, $A_1-B_1$ is the difference between the first feature of each point. $A_n-B_n$ is the difference between the last feature of each point. Using this formula, we can find the K-Nearest Neighbors of a point in N-dimensional space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1cbf7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000000.000008286\n",
      "6000000.000126083\n"
     ]
    }
   ],
   "source": [
    "star_wars = [125, 1977, 11000000]\n",
    "raiders = [115, 1981, 18000000]\n",
    "mean_girls = [97, 2004, 17000000]\n",
    "\n",
    "\n",
    "def distance(movie1, movie2):\n",
    "    squared_difference = 0\n",
    "    for i in range(len(movie1)):\n",
    "        squared_difference += (movie1[i] - movie2[i]) ** 2\n",
    "    final_distance = squared_difference**0.5\n",
    "    return final_distance\n",
    "\n",
    "\n",
    "print(distance(star_wars, raiders))\n",
    "print(distance(star_wars, mean_girls))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d39888f",
   "metadata": {},
   "source": [
    "### **Min-Max Normalisation**\n",
    "\n",
    "The problem is that the distance formula treats all dimensions equally, regardless of their scale. The solution to this problem is to normalize the data so every value is between 0 and 1. In this lesson, we’re going to be using min-max normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a5f63f9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.047619047619047616, 0.8492063492063492, 0.8650793650793651, 0.4523809523809524, 0.5634920634920635, 0.46825396825396826, 0.6666666666666666, 0.5476190476190477, 1.0, 0.36507936507936506, 0.6111111111111112, 0.8333333333333334, 0.42063492063492064, 0.0, 0.8253968253968254, 0.4523809523809524, 0.9523809523809523, 0.5873015873015873, 0.0, 0.6904761904761905]\n"
     ]
    }
   ],
   "source": [
    "release_dates = [\n",
    "    1897.0,\n",
    "    1998.0,\n",
    "    2000.0,\n",
    "    1948.0,\n",
    "    1962.0,\n",
    "    1950.0,\n",
    "    1975.0,\n",
    "    1960.0,\n",
    "    2017.0,\n",
    "    1937.0,\n",
    "    1968.0,\n",
    "    1996.0,\n",
    "    1944.0,\n",
    "    1891.0,\n",
    "    1995.0,\n",
    "    1948.0,\n",
    "    2011.0,\n",
    "    1965.0,\n",
    "    1891.0,\n",
    "    1978.0,\n",
    "]\n",
    "\n",
    "\n",
    "def min_max_normalize(lst):\n",
    "    minimum = min(lst)\n",
    "    maximum = max(lst)\n",
    "    normalized = []\n",
    "\n",
    "    for value in lst:\n",
    "        normalized_num = (value - minimum) / (maximum - minimum)\n",
    "        normalized.append(normalized_num)\n",
    "\n",
    "    return normalized\n",
    "\n",
    "\n",
    "print(min_max_normalize(release_dates))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9f9bb6",
   "metadata": {},
   "source": [
    "### **Classification Process**\n",
    "\n",
    "Now that our data has been normalized and we know how to find the distance between two points, we can begin classifying unknown data!\n",
    "\n",
    "To do this, we want to find the k nearest neighbors of the unclassified point. In a few exercises, we’ll learn how to properly choose k, but for now, let’s choose a number that seems somewhat reasonable. Let’s choose 5.\n",
    "\n",
    "```python\n",
    "def classify(unknown, dataset, k=5):\n",
    "  distances = []\n",
    "  #Looping through all points in the dataset\n",
    "  for title in dataset:\n",
    "    movie = dataset[title]\n",
    "    distance_to_point = distance(movie, unknown)\n",
    "    #Adding the distance and point associated with that distance\n",
    "    distances.append([distance_to_point, title])\n",
    "  distances.sort()\n",
    "  #Taking only the k closest points\n",
    "  neighbors = distances[0:k]\n",
    "```\n",
    "\n",
    "In order to find the 5 nearest neighbors, we need to compare this new unclassified movie to every other movie in the dataset. This means we’re going to be using the distance formula again and again. We ultimately want to end up with a sorted list of distances and the movies associated with those distances.\n",
    "\n",
    "```python\n",
    "[\n",
    "  [0.083, 'Lady Vengeance'],\n",
    "  [0.236, 'Steamboy'],\n",
    "  ...\n",
    "  ...\n",
    "  [0.331, 'Godzilla 2000']\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ab7b94",
   "metadata": {},
   "source": [
    "Our goal now is to count the number of good movies and bad movies in the list of neighbors. If more of the neighbors were good, then the algorithm will classify the unknown movie as good. Otherwise, it will classify it as bad.\n",
    "\n",
    "In order to find the class of each of the labels, we’ll need to look at our movie_labels dataset. For example, `movie_labels['Akira']` would give us 1 because Akira is classified as a good movie.\n",
    "\n",
    "```python\n",
    "def classify(unknown, dataset, labels, k):\n",
    "  distances = []\n",
    "  #Looping through all points in the dataset\n",
    "  for title in dataset:\n",
    "    movie = dataset[title]\n",
    "    distance_to_point = distance(movie, unknown)\n",
    "    #Adding the distance and point associated with that distance\n",
    "    distances.append([distance_to_point, title])\n",
    "  distances.sort()\n",
    "  #Taking only the k closest points\n",
    "  neighbors = distances[0:k]\n",
    "  num_good = 0\n",
    "  num_bad = 0\n",
    "  for neighbor in neighbors:\n",
    "    title = neighbor[1]\n",
    "    if labels[title] == 0:\n",
    "      num_bad += 1\n",
    "    elif labels[title] == 1:\n",
    "      num_good += 1\n",
    "  if num_good > num_bad:\n",
    "    return 1\n",
    "  else:\n",
    "    return 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814c84b8",
   "metadata": {},
   "source": [
    "### **Choosing K**\n",
    "\n",
    "The validation accuracy changes as k changes. The first situation that will be useful to consider is when k is very small. Let’s say `k = 1`. We would expect the validation accuracy to be fairly low due to overfitting. Overfitting is a concept that will appear almost any time you are writing a machine learning algorithm. Overfitting occurs when you rely too heavily on your training data; you assume that data in the real world will always behave exactly like your training data. In the case of K-Nearest Neighbors, overfitting happens when you don’t consider enough neighbors. A single outlier could drastically determine the label of an unknown point. \n",
    "\n",
    "On the other hand, if k is very large, our classifier will suffer from underfitting. Underfitting occurs when your classifier doesn’t pay enough attention to the small quirks in the training set. Imagine you have 100 points in your training set and you set `k = 100`. Every single unknown point will be classified in the same exact way. The distances between the points don’t matter at all! This is an extreme example, however, it demonstrates how the classifier can lose understanding of the training data if k is too big."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ed568c",
   "metadata": {},
   "source": [
    "## **Model Fitting**\n",
    "\n",
    "However, rather than writing your own classifier every time, you can use Python’s `sklearn` library. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f157d11",
   "metadata": {},
   "source": [
    "### **sklearn.neighbors.KNeighborsClassifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425b1b36",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=9)\n",
    "\n",
    "# finds the coefficients and the intercept value\n",
    "knn.fit(X_train, y_train)\n",
    "\n",
    "y_predicted = knn.predict(X_test)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27f80ae",
   "metadata": {},
   "source": [
    "### **sklearn.neighbors.KNeighborsRegressor**\n",
    "\n",
    "The K-Nearest Neighbors algorithm is a powerful supervised machine learning algorithm typically used for classification. However, it can also perform regression. We can compute a weighted average based on how close each neighbor is. Let’s say we’re trying to predict the rating of movie X and we’ve found its three nearest neighbors. Consider the following table:\n",
    "\n",
    "|Movie|Rating|Distance to Movie X|\n",
    "|-|-|-|\n",
    "|A|5.0|3.2|\n",
    "|B|6.8|11.5|\n",
    "|C|9.0|1.1|\n",
    "\n",
    "If we find the mean, the predicted rating for X would be 6.93. However, movie X is most similar to movie C, so movie C’s rating should be more important when computing the average. Using a weighted average, we can find movie X’s rating. The numerator is the sum of every rating divided by their respective distances. The denominator is the sum of one over every distance. Even though the ratings are the same as before, the weighted average has now gone up to 7.9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c0e0145",
   "metadata": {},
   "source": [
    "The `KNeighborsRegressor` class is very similar to `KNeighborsClassifier`. We can also choose whether or not to use a weighted average using the parameter weights. If weights equals \"uniform\", all neighbors will be considered equally in the average. If weights equals \"distance\", then a weighted average is used."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6276d4",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "knn_reg = KNeighborsRegressor(n_neighbors=5, weights=\"distance\")\n",
    "\n",
    "knn_reg.fit(X_train, y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae8cc93",
   "metadata": {},
   "source": [
    "## **Accuracy Curve**\n",
    "\n",
    "```python\n",
    "k_values = list(range(1, 100))\n",
    "k_scores = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    k_scores.append(knn.score(X_test, y_test))\n",
    "\n",
    "sns.lineplot(x=k_values, y=k_scores)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
