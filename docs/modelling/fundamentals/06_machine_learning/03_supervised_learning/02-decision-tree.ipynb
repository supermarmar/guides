{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f792230",
   "metadata": {},
   "source": [
    "# **Decision Trees**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5930a73",
   "metadata": {},
   "source": [
    "Decision trees are a common type of machine learning model used for binary classification tasks. The natural structure of a binary tree lends itself well to predicting a “yes” or “no” target. It is traversed sequentially here by evaluating the truth of each logical statement until the final prediction outcome is reached. Some examples of classification tasks that can use decision trees are: predicting whether a student will pass or fail an exam, whether an email is spam or not, if transaction is fraudulent or legitimate, etc.\n",
    "\n",
    "Decision trees can also be used for regression tasks. As with other scikit-learn models, only numeric data can be used (categorical variables and nulls must be handled prior to model fitting). In this case, our categorical features have already been transformed and no missing values are present in the data set. The syntax is identical as the decision tree classifier, except the target, y, must be real-valued and the model used must be `DecisionTreeRegressor()`. As far as the model hyperparameters go, almost all are the same, except for the split criterion. The split criterion now needs be suitable for a regression task – the default for regression is Mean Squared Error (or MSE)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c2365c",
   "metadata": {},
   "source": [
    "## **Model Fitting**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823a01ef",
   "metadata": {},
   "source": [
    "### **sklearn.tree.DecisionTreeClassifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309fac94",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dtree = DecisionTreeClassifier(max_depth=8, ccp_alpha=0.01, criterion=\"gini\")\n",
    "\n",
    "dtree.fit(X_train, y_train)\n",
    "y_predicted = dtree.predict(X_test)\n",
    "\n",
    "dtree.feature_importances_\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e62b7b4",
   "metadata": {},
   "source": [
    "## **Visualizing Decision Trees**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba316c",
   "metadata": {},
   "source": [
    "Two methods are available to visualize the tree within the tree module – the first is using `tree_plot` to graphically represent the decision tree. \n",
    "\n",
    "```python\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn import tree\n",
    "\n",
    "plt.figure(figsize=(20, 12))\n",
    "tree.plot_tree(\n",
    "    dtree,\n",
    "    feature_names=X_train.columns,\n",
    "    max_depth=5,\n",
    "    class_names=[\"Drowned\", \"Survived\"],\n",
    "    label=\"all\",\n",
    "    filled=True,\n",
    ")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da5f5f6",
   "metadata": {},
   "source": [
    "The second uses `export_text` to list the rules behind the splits in the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18663015",
   "metadata": {},
   "source": [
    "```python\n",
    "print(tree.export_text(dtree, feature_names=X_train.columns))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefd2939",
   "metadata": {},
   "source": [
    "### **sklearn.tree.DecisionTreeClassifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0635dce",
   "metadata": {},
   "source": [
    "```python\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dt = DecisionTreeRegressor(max_depth=3, ccp_alpha=0.001)\n",
    "dt.fit(X_iris_train, y_iris_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1973fc0",
   "metadata": {},
   "source": [
    "Similarly, the tree can be visualized using `tree.plot_tree` – keeping in mind the splitting criteria is mse and the value is the average target variables of all samples in that leaf."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2707f21",
   "metadata": {},
   "source": [
    "```python\n",
    "plt.figure(figsize=(20, 12))\n",
    "tree.plot_tree(dt, feature_names=X_iris_train.columns, max_depth=4, filled=True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9730b2f2",
   "metadata": {},
   "source": [
    "Decision trees are easy to understand, fully explainable, and have a natural way to visualize the decision making process. In addition, often little modification needs to be made to the data prior to modeling (such as scaling, normalization, removing outliers) and decision trees are relatively quick to train and predict. However, now let’s talk about some of their limitations.\n",
    "\n",
    "One problem with the way we’re currently making our decision trees is that our trees aren’t always globally optimal. This means that there might be a better tree out there somewhere that produces better results. But wait, why did we go through all that work of finding information gain if it’s not producing the best possible tree?\n",
    "\n",
    "Our current strategy of creating trees is greedy. We assume that the best way to create a tree is to find the feature that will result in the largest information gain right now and split on that feature. We never consider the ramifications of that split further down the tree. It’s possible that if we split on a suboptimal feature right now, we would find even better splits later on. Unfortunately, finding a globally optimal tree is an extremely difficult task, and finding a tree using our greedy approach is a reasonable substitute.\n",
    "\n",
    "Another problem with our trees is that they are prone to overfit the data. This means that the structure of the tree is too dependent on the training data and may not generalize well to new data. In general, larger trees tend to overfit the data more. As the tree gets bigger, it becomes more tuned to the training data and it loses a more generalized understanding of the real world data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c1fb5",
   "metadata": {},
   "source": [
    "## **Gini Impurity**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710dbe25",
   "metadata": {},
   "source": [
    "The root node is identified as the top of the tree. This is notated already with the number of samples and the numbers in each class (i.e. True vs. False) that was used to build the tree. Splits occur with True to the left, False to the right. Note the right split is a leaf node i.e., there are no more branches. Any decision ending here results in the majority class. (The majority class here is False.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689e0d90",
   "metadata": {},
   "source": [
    "This idea can be quantified by calculating the Gini impurity of a set of data points. For two classes (1 and 2) with probabilites $p_1$ and $p_2$ respectively, the Gini impurity is:\n",
    "\n",
    "$1-(p_1^2 + p_2^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd04147",
   "metadata": {},
   "source": [
    "The goal of a decision tree model is to separate the classes the best possible, i.e. minimize the impurity (or maximize the purity). Notice that if $p_1$ is 0 or 1, the Gini impurity is 0, which means there is only one class so there is perfect separation. From the graph, the Gini impurity is maximum at $p_1=0.5$, which means the two classes are equally balanced, so this is perfectly impure!\n",
    "\n",
    "In general, the Gini impurity for C classes is defined as:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aff1407",
   "metadata": {},
   "source": [
    "$1-\\Sigma p_i^2$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f40d1cf4",
   "metadata": {},
   "source": [
    "## **Information Gain**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc21fa80",
   "metadata": {},
   "source": [
    "For a classification task, the default split criteria is Gini impurity – this gives us a measure of how “impure” the groups are. At the root node, the first split is then chosen as the one that maximizes the information gain, i.e. decreases the Gini impurity the most. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb2b7f34",
   "metadata": {},
   "source": [
    "We know that we want to end up with leaves with a low Gini Impurity, but we still need to figure out which features to split on in order to achieve this. To answer this question, we can calculate the information gain of splitting the data on a certain feature. Information gain measures the difference in the impurity of the data before and after the split."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
