{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b90ea6b",
   "metadata": {},
   "source": [
    "# **Pipeline**\n",
    "\n",
    "Scikit-learn’s `Pipeline` class enables you to combine different preprocessors or models into a single, callable chunk of code:\n",
    "\n",
    "![image](images/pipeline.png)\n",
    "\n",
    "Pipelines can be composed of two different things:\n",
    "1. Transformer: any object with the fit() and transform() methods. You can think of a transformer as an object that’s used for processing your data, and you will commonly have multiple transformers in your data preparation workflow. E.g., you might use one transformer to impute missing values, and another one to scale features or one-hot encode your categorical variables. MinMaxScaler(), SimpleImputer() and OneHotEncoder() are all examples of transformers.\n",
    "2. Estimator: In scikit-learn lingo, an “estimator” usually means a machine learning model; i.e. an object with the fit() and predict() methods. LinearRegression() and RandomForestClassifier() are examples of estimators.\n",
    "In a pipeline, you can chain together as many transformers as you like, enabling you to apply different data preprocessing steps sequentially. If you like, you can also add on an estimator (ML model) at the end in order to make predictions using the newly transformed data, but it’s not compulsory.\n",
    "\n",
    "For example, you could build a pipeline that first imputes missing values with zeros and then one-hot encodes your variables:\n",
    "\n",
    "![image](images/pipeline_example_1.png)\n",
    "\n",
    "Or, if you wanted to directly include the modelling in the pipeline itself, you could build a pipeline that imputes missing values with the mean, scales the features and then makes predictions using a `RandomForestRegressor()`:\n",
    "\n",
    "![image](images/pipeline_example_2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63a779b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "age",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "sex",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "bmi",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "bp",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "s1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "s2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "s3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "s4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "s5",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "s6",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "f96268c0-8980-44fc-80eb-92aee0d2da80",
       "rows": [
        [
         "19",
         "41.0",
         "1.0",
         "24.7",
         "83.0",
         "187.0",
         "108.2",
         "60.0",
         "3.0",
         "4.5433",
         "78.0"
        ],
        [
         "90",
         "52.0",
         "1.0",
         "24.0",
         "83.0",
         "167.0",
         "86.6",
         "71.0",
         "2.0",
         "3.8501",
         "94.0"
        ],
        [
         "158",
         "45.0",
         "1.0",
         "20.3",
         "74.33",
         "190.0",
         "126.2",
         "49.0",
         "3.88",
         "4.3041",
         "79.0"
        ],
        [
         "256",
         "35.0",
         "1.0",
         "41.3",
         "81.0",
         "168.0",
         "102.8",
         "37.0",
         "5.0",
         "4.9488",
         "94.0"
        ],
        [
         "288",
         "68.0",
         "2.0",
         "24.8",
         "101.0",
         "221.0",
         "151.4",
         "60.0",
         "4.0",
         "3.8712",
         "87.0"
        ]
       ],
       "shape": {
        "columns": 10,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>bmi</th>\n",
       "      <th>bp</th>\n",
       "      <th>s1</th>\n",
       "      <th>s2</th>\n",
       "      <th>s3</th>\n",
       "      <th>s4</th>\n",
       "      <th>s5</th>\n",
       "      <th>s6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>41.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.7</td>\n",
       "      <td>83.00</td>\n",
       "      <td>187.0</td>\n",
       "      <td>108.2</td>\n",
       "      <td>60.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.5433</td>\n",
       "      <td>78.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>24.0</td>\n",
       "      <td>83.00</td>\n",
       "      <td>167.0</td>\n",
       "      <td>86.6</td>\n",
       "      <td>71.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.8501</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>45.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>20.3</td>\n",
       "      <td>74.33</td>\n",
       "      <td>190.0</td>\n",
       "      <td>126.2</td>\n",
       "      <td>49.0</td>\n",
       "      <td>3.88</td>\n",
       "      <td>4.3041</td>\n",
       "      <td>79.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>41.3</td>\n",
       "      <td>81.00</td>\n",
       "      <td>168.0</td>\n",
       "      <td>102.8</td>\n",
       "      <td>37.0</td>\n",
       "      <td>5.00</td>\n",
       "      <td>4.9488</td>\n",
       "      <td>94.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>68.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>24.8</td>\n",
       "      <td>101.00</td>\n",
       "      <td>221.0</td>\n",
       "      <td>151.4</td>\n",
       "      <td>60.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3.8712</td>\n",
       "      <td>87.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      age  sex   bmi      bp     s1     s2    s3    s4      s5    s6\n",
       "19   41.0  1.0  24.7   83.00  187.0  108.2  60.0  3.00  4.5433  78.0\n",
       "90   52.0  1.0  24.0   83.00  167.0   86.6  71.0  2.00  3.8501  94.0\n",
       "158  45.0  1.0  20.3   74.33  190.0  126.2  49.0  3.88  4.3041  79.0\n",
       "256  35.0  1.0  41.3   81.00  168.0  102.8  37.0  5.00  4.9488  94.0\n",
       "288  68.0  2.0  24.8  101.00  221.0  151.4  60.0  4.00  3.8712  87.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "target",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "231bf69a-f857-4283-be43-a2429a6bcc54",
       "rows": [
        [
         "19",
         "168.0"
        ],
        [
         "90",
         "98.0"
        ],
        [
         "158",
         "96.0"
        ],
        [
         "256",
         "346.0"
        ],
        [
         "288",
         "80.0"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 5
       }
      },
      "text/plain": [
       "19     168.0\n",
       "90      98.0\n",
       "158     96.0\n",
       "256    346.0\n",
       "288     80.0\n",
       "Name: target, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load diabetes dataset into pandas DataFrames\n",
    "X, y = load_diabetes(scaled=False, return_X_y=True, as_frame=True)\n",
    "\n",
    "# Split into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474f0d50",
   "metadata": {},
   "source": [
    "## **sklearn.pipeline.Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8492e88",
   "metadata": {},
   "source": [
    "Next, we define our `Pipeline`. For now, I’ll just define a simple preprocessing `Pipeline` that includes two steps — impute missing values with the mean, and rescale all features — and I won’t include an estimator/model. The principles, however, are the same regardless of whether or not you include an estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e543df1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Return pandas DataFrames instead of numpy arrays\n",
    "set_config(transform_output=\"pandas\")\n",
    "\n",
    "# Build pipeline\n",
    "pipe = Pipeline(\n",
    "    steps=[(\"impute_mean\", SimpleImputer(strategy=\"mean\")), (\"rescale\", MinMaxScaler())]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4fb5db",
   "metadata": {},
   "source": [
    "Once we’ve defined our `Pipeline`, we “fit” it to our training dataset, and use it to transform both the training and testing datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3adb8e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the pipeline to the training data\n",
    "pipe.fit(X_train)\n",
    "\n",
    "# Transform data using the fitted pipeline\n",
    "X_train_transformed = pipe.transform(X_train)\n",
    "X_test_transformed = pipe.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeb68ee",
   "metadata": {},
   "source": [
    "This will give us two preprocessed datasets (`X_train_transformed` and `X_test_transformed`), ready for any subsequent steps like modelling or feature selection.\n",
    "\n",
    "The advantage of using a `Pipeline` to handle these preprocessing steps is twofold:\n",
    "- Protect against leakage: Because the preprocessor is fitted to the training dataset X_train, no information about the test set is “leaked” when imputing missing values or creating one-hot encoded features.\n",
    "- Avoid duplication: If we didn’t use a `Pipeline` to handle these preprocessing steps, we’d end up transforming the `X_test` dataset multiple times (every time we wanted to apply a preprocessing step). At this small scale, the repetition might not seem too bad. But in complex ML workflows you can easily grow to 5, 10, or even 20 preprocessing steps. Using a `Pipeline` makes this easy because we can add in as many steps as we like and still only have to transform `X_train` and `X_test` once."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fc6ebdc",
   "metadata": {},
   "source": [
    "# **Extra Reading**\n",
    "\n",
    "https://medium.com/data-science/simplify-your-data-preparation-with-these-4-lesser-known-scikit-learn-classes-70270c94569f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "src-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
