{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c019359",
   "metadata": {},
   "source": [
    "\n",
    "# **Principle Component Analysis**\n",
    "\n",
    "In the world of Machine Learning, any model that we implement will be more valuable when the features are engineered to suit the question we’re trying to answer. With many datasets, we can simply include all available features, which gives us the full picture about our observations. For example, it’s straightforward to see a correlation between height and weight for a patient dataset. Some datasets, however, have very large numbers of features. If our example patient dataset expanded to include 20 different features, how would we visualize and correlate this data? When it comes time to actually process the data and train the model, we often hit computational or complexity limits. How do we leverage correlations within the data to make fewer, better features without losing the information included in the dataset?\n",
    "\n",
    "Situations like this are a great use case for implementing Principal Component Analysis. PCA is a technique where we can reduce the number of features in a dataset without losing any of the information we have. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9f53c6",
   "metadata": {},
   "source": [
    "## **Math Behind PCA**\n",
    "\n",
    "Essentially, a covariance matrix is calculating how much a feature changes with changes in every other feature, i.e., we’re looking at the relative variance between any two features.\n",
    "\n",
    "We now have a matrix of variance values for our features. The next step in PCA revolves around matrix factorization. Without going into too much detail, our goal with matrix factorization is to find a pair of smaller matrices whose product would equal our covariance matrix. Another way of thinking about it: we want to find a smaller matrix that captures the majority of our information.\n",
    "\n",
    "An important part of this matrix factorization are Eigenvectors. Eigenvectors are vectors (mathematical concepts that have direction and magnitude) that do not change direction when a transformation is applied to them. In the context of data matrices, these eigenvectors give us a direction to “rotate” the dataset in n-dimensional space so we can look at the entire dataset from a simplified perspective. The eigenvalues are related to the relative variation described by each principal component.\n",
    "\n",
    "All of the underlying math behind PCA results in principal components, but what exactly are they? Principal components are a linear combination of all the input features from the original dataset. By using the eigenvectors we calculated earlier, we can “rotate” our dataset features from an n-dimensional space into a 2-dimensional space, which is easier for us to understand and analyze.\n",
    "\n",
    "If we wanted to, however, look at all of the feature relationships and information at once, it would be very difficult to decipher, as we cannot visualize data in a 5-dimensional space.\n",
    "\n",
    "By using PCA, however, we can reduce the dimensionality of our dataset into a 2-dimensional dataset, allowing for better visualization.\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_array = pca.fit_transform(df)\n",
    "```\n",
    "\n",
    "As we can see, by running PCA on our original dataset, we were able to take our 5 features and reduce the dimensions down to 2 principal components. With 2 dimensions, we can now plot the data on a single scatterplot:\n",
    "\n",
    "```python\n",
    "sns.scatterplot(pca_array[:,0], pca_array[:,1])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "546b3ead",
   "metadata": {},
   "source": [
    "\n",
    "## **Coeffcient of Variance**\n",
    "\n",
    "Variance alone is one indicator of the level of information in a dataset, but is not the only factor. To expand on the idea of variance within a dataset, we will look at the Coefficient of Variance, or CV for short. The premise here is that variance must be taken into context with the central tendencies of that dataset. For example, if a dataset has a variance of 5, that will mean very different things if the mean is 2 vs. a dataset with a mean of 100.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "#define function to calculate cv\n",
    "cv = lambda x: np.std(x, ddof=1) / np.mean(x) * 100\n",
    "\n",
    "print(df.apply(cv))\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6091cf13",
   "metadata": {},
   "source": [
    "## **Use Cases**\n",
    "\n",
    "PCA is also inherently an unsupervised learning algorithm and can be used to identify clusters in data on its own. Very similar to the popular k-means algorithms, PCA will look at overall similarities between the different features in a dataset. When we set the number of principal components to keep, we are defining the number of similar “rotations” of our dataset, which will act very much like a cluster of their own. Typically, many practitioners will implement PCA as a precursor to other clustering algorithms to augment the accuracy, but it is an interesting application to do clustering with PCA alone!\n",
    "\n",
    "In particular, we have seen how:\n",
    "\n",
    "1. Implementation: scikit-learn provides a more in-depth set of methods and attributes that extend the number of ways to perform PCA or display the percentage of variance for each principal axis.\n",
    "2. Dimensionality reduction: We visualized the data projected onto the principal axes, known as principal components.\n",
    "3. Image classification: We performed PCA on images of faces to visually understand how principal components still retain nearly all the information in the original dataset.\n",
    "4. Improved algorithmic speed/accuracy: Using principal components as input to a classifier, we observed how we could achieve equal or better results with lower dimensional data. Having lower dimensionality also speeds the training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16104088",
   "metadata": {},
   "source": [
    "![image](images/pca_use_cases.png)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
