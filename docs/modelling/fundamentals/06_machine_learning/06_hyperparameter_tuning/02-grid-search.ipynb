{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6005bc56",
   "metadata": {},
   "source": [
    "# **Grid Search Algorithm**\n",
    "\n",
    "The grid search algorithm for hyperparameter tuning works by training a model on predetermined lists of hyperparameter values. This method tries every hyperparameter value on the list, and then uses the one that makes the model perform best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0279262",
   "metadata": {},
   "source": [
    "Suppose we had two hyperparameters we wanted to tune and we wanted to choose between 6 values for the first one and 5 values of the second, we’d be searching a grid of thirty values as shown below. Grid search would fit the model and evaluate its performance for each of the values represented by these points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75859031",
   "metadata": {},
   "source": [
    "![image](images/grid_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d226346",
   "metadata": {},
   "source": [
    "## **sklearn.model_selection.GridSearchCV**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213b47a0",
   "metadata": {},
   "source": [
    "The two most important parameters in `GridSearchCV` that need to be specified are: the name of the model that we are testing and the name of a dictionary of hyperparameters that we would initialize, represented by the argument `parameters`. To tune the hyperparameters, we can use `fit()`, just as we would for a regular machine learning model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fab8d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 1.0,\n",
       " 'class_weight': None,\n",
       " 'dual': False,\n",
       " 'fit_intercept': True,\n",
       " 'intercept_scaling': 1,\n",
       " 'l1_ratio': None,\n",
       " 'max_iter': 1000,\n",
       " 'multi_class': 'deprecated',\n",
       " 'n_jobs': None,\n",
       " 'penalty': 'l2',\n",
       " 'random_state': None,\n",
       " 'solver': 'liblinear',\n",
       " 'tol': 0.0001,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "\n",
    "# Load the data set\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# Split into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target)\n",
    "\n",
    "lr = LogisticRegression(solver=\"liblinear\", max_iter=1000)\n",
    "\n",
    "lr.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41307064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cv': None,\n",
       " 'error_score': nan,\n",
       " 'estimator__C': 1.0,\n",
       " 'estimator__class_weight': None,\n",
       " 'estimator__dual': False,\n",
       " 'estimator__fit_intercept': True,\n",
       " 'estimator__intercept_scaling': 1,\n",
       " 'estimator__l1_ratio': None,\n",
       " 'estimator__max_iter': 1000,\n",
       " 'estimator__multi_class': 'deprecated',\n",
       " 'estimator__n_jobs': None,\n",
       " 'estimator__penalty': 'l2',\n",
       " 'estimator__random_state': None,\n",
       " 'estimator__solver': 'liblinear',\n",
       " 'estimator__tol': 0.0001,\n",
       " 'estimator__verbose': 0,\n",
       " 'estimator__warm_start': False,\n",
       " 'estimator': LogisticRegression(max_iter=1000, solver='liblinear'),\n",
       " 'n_jobs': None,\n",
       " 'param_grid': {'penalty': ['l1', 'l2'], 'C': [1, 10, 100]},\n",
       " 'pre_dispatch': '2*n_jobs',\n",
       " 'refit': True,\n",
       " 'return_train_score': False,\n",
       " 'scoring': None,\n",
       " 'verbose': 0}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "parameters = {\"penalty\": [\"l1\", \"l2\"], \"C\": [1, 10, 100]}\n",
    "clf = GridSearchCV(lr, parameters)\n",
    "clf.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb23c40a",
   "metadata": {},
   "source": [
    "## **Cross-validation**\n",
    "\n",
    "The “CV” in `GridSearchCV` is an acronym for cross-validation. It’s best practice in machine learning to go beyond the usual train-test split and have a holdout or validation dataset. Specifically, GridSearhCV uses a technique known as k-fold cross-validation. This works as follows.\n",
    "\n",
    "GridSearchCV subdivides the training data further into another training and test data set. It fits the model on this new training data and evaluates the model on the new test data. But to make sure that we don’t accidentally have good performance in only one part of our dataset, GridSearchCV will do this process multiple times on different cross-validation splits so that every point in the data gets to be tested on at least once! The number of times this split happens is the “k” in “k-fold”. For instance, in a 10-fold cross-validation, our data would be split into a 90:10 train-test split 10 times and GridSearchCV would evaluate the model on each fold."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e378a9",
   "metadata": {},
   "source": [
    "In scikit-learn, `cv` argument in GridSearchCV allows us to decide on the number of cross-validation splits we’d like. The default setting for this is 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340ecd68",
   "metadata": {},
   "source": [
    "After fitting a GridSearchCV model we can find out the results using the following attributes of the `clf` argument:\n",
    "\n",
    "- `.best_estimator_` gives us the best estimator\n",
    "- `.best_score_` gives us the mean cross-validated score corresponding to the best estimator\n",
    "- `.best_params_` gives us the set of hyperparameters that correspond to the best estimator\n",
    "\n",
    "Additionally, the `.cv_results_` attribute gives us the scores for each hyperparamter combination in the grid. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf17674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=1, max_iter=1000, penalty='l1', solver='liblinear')\n",
      "{'C': 1, 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "best_model = clf.best_estimator_\n",
    "print(best_model)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1c88dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9530779753761971\n",
      "0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "best_score = clf.best_score_\n",
    "test_score = clf.score(X_test, y_test)\n",
    "print(best_score)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53805217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     C penalty     score\n",
      "0    1      l1  0.953078\n",
      "1    1      l2  0.943748\n",
      "2   10      l1  0.948427\n",
      "3   10      l2  0.946074\n",
      "4  100      l1  0.950807\n",
      "5  100      l2  0.948427\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hyperparameter_grid = pd.DataFrame(clf.cv_results_[\"params\"])\n",
    "grid_scores = pd.DataFrame(clf.cv_results_[\"mean_test_score\"], columns=[\"score\"])\n",
    "\n",
    "df = pd.concat([hyperparameter_grid, grid_scores], axis=1)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c958cf",
   "metadata": {},
   "source": [
    "# **Extra Reading**\n",
    "\n",
    "https://medium.com/data-science/its-a-mistake-to-trust-the-best-model-of-a-gridsearchcv-536a73e835ad\n",
    "\n",
    "https://towardsdatascience.com/a-highly-anticipated-time-series-cross-validator-is-finally-here-7dc99f672736/?source=post_page-----536a73e835ad---------------------------------------\n",
    "\n",
    "https://medium.com/@abhishekjainindore24/optuna-vs-gridsearchcv-vs-randomsearchcv-hyperparameter-tuning-techniques-ea8e2ada28d0\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
