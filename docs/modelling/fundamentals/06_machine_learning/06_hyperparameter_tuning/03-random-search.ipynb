{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c408968",
   "metadata": {},
   "source": [
    "\n",
    "# **Random Search Algorithm**\n",
    "\n",
    "The random search algorithm works similarly, but instead of using a predetermined list of hyperparameter values, the values are randomly chosen. As with grid search, it selects the hyperparameter that performed the best.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea79ce",
   "metadata": {},
   "source": [
    "![image](images/random_search.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7d81f6",
   "metadata": {},
   "source": [
    "To implement this, one needs to search over hyperparameters by drawing a list of random values from distributions. What would this mean for say, the hyperparameter `C` in logistic regression that we’ve been working with so far? Instead of selecting from a pre-determined list (like `[1,10,100]` which we’ve been using for grid search in previous exercises), we would draw 3 random values between say 0 and 100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bfcf31",
   "metadata": {},
   "source": [
    "In scikit-learn, the `RandomSearchCV` function implements random search with cross-validation. `RandomSearchCV` requires three arguments to be specified:\n",
    "\n",
    "- `estimator`: the machine learning model whose hyperparameters we’re tuning; this is exactly the same for GridSearchCV\n",
    "- `param_distributions`: a dictionary which specifies the hyperparameters as keys and corresponding distributions to draw lists of values from for each hyperparameter. In GridSearchCV, we instead had param_grid, a dictionary representing the grid of hyperparameters to search from\n",
    "- `n_iter`: the number of times the algorithm needs to randomly draw from the distributions. The default value for this is 10."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f941d8",
   "metadata": {},
   "source": [
    "With random search, we don’t have to make a list, but we still have to provide some information about how we want to select random numbers. Do we want random numbers between 0 and 100? Between -10 and 10? Do we want the same chance of picking small numbers and picking large numbers?\n",
    "\n",
    "We can do this by specifying a probability distribution for each hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696a6196",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "\n",
    "distributions = {\"penalty\": [\"l1\", \"l2\"], \"C\": uniform(loc=0, scale=100)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cd562b",
   "metadata": {},
   "source": [
    "- The `penalty` hyperparameter of scikit-learn’s `LogisticRegression` model has only two possible values: l1 and l2. We list them both. `RandomizedSearchCV` will treat this as a discrete uniform distribution. This just means that every item in the list has an equal chance of being selected. In this case, there’s a 50% chance of drawing l1 and a 50% chance of drawing l2.\n",
    "- The hyperparameter `C` is the inverse of regularization strength. It can be any positive number, so we have to specify a probability distribution that allows us to randomly select a positive number. The `scipy` library has many probability distributions to choose from. For this example, we’re using the uniform distribution. This allows us to randomly select numbers between `loc` and `loc + scale` (in this case, between 0 and 100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf719eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[98.17561301 15.68609911 66.25615582 95.87047921 15.12271514 20.24596147\n",
      " 56.92515023 82.20232609 63.76772312 89.40235078]\n",
      "[95.04578112 33.89459268 77.55846237  2.17034458 39.35571902  7.83673503\n",
      " 17.36575256 86.49431442 10.55323588 22.93857171]\n"
     ]
    }
   ],
   "source": [
    "first_draw = distributions[\"C\"].rvs(10)\n",
    "second_draw = distributions[\"C\"].rvs(10)\n",
    "print(first_draw)\n",
    "print(second_draw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a9bc7d",
   "metadata": {},
   "source": [
    "## **sklearn.model_selection.RandomizedSearchCV**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa028d7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "\n",
    "# Load the data set\n",
    "cancer = load_breast_cancer()\n",
    "\n",
    "# Split into training and testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(cancer.data, cancer.target)\n",
    "\n",
    "lr = LogisticRegression(solver=\"liblinear\", max_iter=1000)\n",
    "\n",
    "# Create a RandomizedSearchCV model\n",
    "clf = RandomizedSearchCV(lr, distributions, n_iter=8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769aff42",
   "metadata": {},
   "source": [
    "After fitting a `RandomizedSearchCV` model we can find out the results using the following attributes of the `clf` argument:\n",
    "\n",
    "- `.best_estimator_` gives us the best estimator\n",
    "- `.best_score_` gives us the mean cross-validated score corresponding to the best estimator\n",
    "- `.best_params_` gives us the set of hyperparameters that correspond to the best estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85f53ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression(C=np.float64(62.90838954241394), max_iter=1000, penalty='l1',\n",
      "                   solver='liblinear')\n",
      "{'C': np.float64(62.90838954241394), 'penalty': 'l1'}\n"
     ]
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "best_model = clf.best_estimator_\n",
    "print(best_model)\n",
    "print(clf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a543e48d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9554856361149111\n",
      "0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "best_score = clf.best_score_\n",
    "test_score = clf.score(X_test, y_test)\n",
    "print(best_score)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be8ff33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           C penalty     score\n",
      "0  52.263196      l2  0.950780\n",
      "1  62.908390      l1  0.955486\n",
      "2  61.419905      l1  0.955486\n",
      "3  97.818871      l1  0.950807\n",
      "4  83.886440      l1  0.955458\n",
      "5  77.980712      l2  0.948427\n",
      "6  54.864810      l1  0.955486\n",
      "7  53.429513      l2  0.948427\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "hyperparameter_values = pd.DataFrame(clf.cv_results_[\"params\"])\n",
    "randomsearch_scores = pd.DataFrame(clf.cv_results_[\"mean_test_score\"], columns=[\"score\"])\n",
    "\n",
    "df = pd.concat([hyperparameter_values, randomsearch_scores], axis=1)\n",
    "print(df)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
