{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **DDL (Definition)** 🆔"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-20 16:12:13.993\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mconfig\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mPROJ_ROOT path is: C:\\Users\\mario\\OneDrive\\Documents\\Education\\guide_repo\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import duckdb as dd\n",
    "from faker import Faker\n",
    "import pandas as pd\n",
    "\n",
    "from config import CREDIT_RISK_DATA_DIR, DATABASE_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **DuckDB Connection**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In DuckDB, databases are either stored as files or kept `in memory`, which can be created like this. This command creates an in-memory database, meaning all operations are performed in the memory and won’t persist after the session ends.\n",
    "\n",
    "```python\n",
    "# Create an in-memory DuckDB connection\n",
    "con = dd.connect(':memory:')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can create a persistent DuckDB database by specifying a file path. This stores the database in the on-disk file `duckdb_test.db` in the current working directory, making it persistent between sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a persistent DuckDB database\n",
    "os.chdir(DATABASE_DIR)\n",
    "con = dd.connect(\"duckdb_test.db\")\n",
    "\n",
    "FILE_PATH = Path(CREDIT_RISK_DATA_DIR).as_posix()\n",
    "INPUT_FILE = \"credit_risk_dataset\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DuckDB supports standard SQL syntax, so you can run any SQL query with ease. Let’s start by creating an on-file/persistent database and querying it to see it’s contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you’re done with your queries, always remember to close the DuckDB connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Creating Tables** \n",
    "\n",
    "The CREATE TABLE statement creates a new table in a database. It allows one to specify the name of the table and the name of each column in the table. In a CREATE TABLE statement we specify the data type for each column of a table (e.g., int, text, timestamp, etc.). Column constraints are the rules applied to the values of individual columns:\n",
    "\n",
    "- PRIMARY KEY constraint can be used to uniquely identify the row.\n",
    "- NOT NULL columns must have a value.\n",
    "- CHECK columns must meet a pre-defined Boolean expression.\n",
    "- UNIQUE columns have a different value for every row.\n",
    "- DEFAULT assigns a default value for the column when no value is specified.\n",
    "\n",
    "The SERIAL datatype creates an autoincrementing column that can be used as an Primary Key column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x22a67b06fb0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\n",
    "    \"\"\"\n",
    "    DROP TABLE IF EXISTS storage;            \n",
    "    DROP TABLE IF EXISTS users;            \n",
    "\n",
    "    CREATE OR REPLACE TABLE users (\n",
    "        id INTEGER PRIMARY KEY,\n",
    "        name TEXT NOT NULL,\n",
    "        age INTEGER CHECK (age > 0),\n",
    "        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "        email TEXT UNIQUE\n",
    "    );\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x22a67b06fb0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.execute(\n",
    "    \"\"\"\n",
    "    DROP TABLE IF EXISTS reporting;  \n",
    "\n",
    "    CREATE OR REPLACE TABLE reporting (\n",
    "        person_age INT,\n",
    "        person_income INT,\n",
    "        person_home_ownership VARCHAR,\n",
    "        person_emp_length FLOAT,\n",
    "        loan_intent VARCHAR,\n",
    "        loan_grade VARCHAR,\n",
    "        loan_amnt INT,\n",
    "        loan_int_rate FLOAT,\n",
    "        loan_status INT,\n",
    "        loan_percent_income FLOAT,\n",
    "        cb_person_default_on_file VARCHAR,\n",
    "        cb_person_cred_hist_length INT\n",
    "    );\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Ingesting Tables**\n",
    "\n",
    "We can ingest into tables from Pandas or Polars dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "age",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "created_at",
         "rawType": "datetime64[ns]",
         "type": "datetime"
        },
        {
         "name": "email",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "e6103535-7bf5-47e3-90a0-bba4bb5b4e59",
       "rows": [
        [
         "0",
         "0",
         "John Vincent",
         "52",
         "2025-05-19 22:28:17",
         "john.vincent@example.com"
        ],
        [
         "1",
         "1",
         "Jessica Carey",
         "67",
         "2021-08-29 08:22:24",
         "jessica.carey@example.com"
        ],
        [
         "2",
         "2",
         "David Pratt",
         "31",
         "2024-06-24 20:48:34",
         "david.pratt@example.com"
        ],
        [
         "3",
         "3",
         "Jennifer Allen",
         "69",
         "2024-10-12 15:37:12",
         "jennifer.allen@example.com"
        ],
        [
         "4",
         "4",
         "Cory Baker",
         "30",
         "2021-04-10 08:32:59",
         "cory.baker@example.com"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>created_at</th>\n",
       "      <th>email</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>John Vincent</td>\n",
       "      <td>52</td>\n",
       "      <td>2025-05-19 22:28:17</td>\n",
       "      <td>john.vincent@example.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Jessica Carey</td>\n",
       "      <td>67</td>\n",
       "      <td>2021-08-29 08:22:24</td>\n",
       "      <td>jessica.carey@example.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>David Pratt</td>\n",
       "      <td>31</td>\n",
       "      <td>2024-06-24 20:48:34</td>\n",
       "      <td>david.pratt@example.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Jennifer Allen</td>\n",
       "      <td>69</td>\n",
       "      <td>2024-10-12 15:37:12</td>\n",
       "      <td>jennifer.allen@example.com</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Cory Baker</td>\n",
       "      <td>30</td>\n",
       "      <td>2021-04-10 08:32:59</td>\n",
       "      <td>cory.baker@example.com</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id            name  age          created_at                       email\n",
       "0   0    John Vincent   52 2025-05-19 22:28:17    john.vincent@example.com\n",
       "1   1   Jessica Carey   67 2021-08-29 08:22:24   jessica.carey@example.com\n",
       "2   2     David Pratt   31 2024-06-24 20:48:34     david.pratt@example.com\n",
       "3   3  Jennifer Allen   69 2024-10-12 15:37:12  jennifer.allen@example.com\n",
       "4   4      Cory Baker   30 2021-04-10 08:32:59      cory.baker@example.com"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize the Faker object\n",
    "fake = Faker()\n",
    "sample = 100\n",
    "\n",
    "# Create a dummy data set\n",
    "dummy_data = {\n",
    "    \"name\": [fake.name() for _ in range(sample)],\n",
    "    \"age\": [fake.random_int(min=18, max=80) for _ in range(sample)],\n",
    "    \"created_at\": [fake.date_time_this_decade() for _ in range(sample)],\n",
    "}\n",
    "\n",
    "# Convert the dummy data to a DataFrame\n",
    "dummy_df = pd.DataFrame(dummy_data)\n",
    "dummy_df[\"email\"] = dummy_df.apply(\n",
    "    lambda x: f\"{x['name'].replace(' ', '.').lower()}@example.com\", axis=1\n",
    ")\n",
    "dummy_df.reset_index(inplace=True, names=\"id\")\n",
    "dummy_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x22a67b06fb0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    INSERT INTO users BY NAME \n",
    "    SELECT * FROM dummy_df\n",
    "\"\"\"\n",
    "\n",
    "con.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the examples above we saw, DuckDB gives us the capability to create tables and allows us to manually add data to them. However, if we are talking about large sets of data, we can ingest data from a variety of sources, including CSV, Parquet, JSON, etc. files. DuckDB lets us capture and store this data in a database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x22a67c49ff0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = f\"\"\"\n",
    "    INSERT INTO reporting BY NAME \n",
    "    SELECT * FROM \"{FILE_PATH}/{INPUT_FILE}.csv\"\n",
    "\"\"\"\n",
    "\n",
    "con.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌────────────┬───────────────┬───────────────────────┬───────────────────┬─────────────┬────────────┬───────────┬───────────────┬─────────────┬─────────────────────┬───────────────────────────┬────────────────────────────┐\n",
       "│ person_age │ person_income │ person_home_ownership │ person_emp_length │ loan_intent │ loan_grade │ loan_amnt │ loan_int_rate │ loan_status │ loan_percent_income │ cb_person_default_on_file │ cb_person_cred_hist_length │\n",
       "│   int32    │     int32     │        varchar        │       float       │   varchar   │  varchar   │   int32   │     float     │    int32    │        float        │          varchar          │           int32            │\n",
       "├────────────┼───────────────┼───────────────────────┼───────────────────┼─────────────┼────────────┼───────────┼───────────────┼─────────────┼─────────────────────┼───────────────────────────┼────────────────────────────┤\n",
       "│         22 │         59000 │ RENT                  │             123.0 │ PERSONAL    │ D          │     35000 │         16.02 │           1 │                0.59 │ Y                         │                          3 │\n",
       "│         21 │          9600 │ OWN                   │               5.0 │ EDUCATION   │ B          │      1000 │         11.14 │           0 │                 0.1 │ N                         │                          2 │\n",
       "│         25 │          9600 │ MORTGAGE              │               1.0 │ MEDICAL     │ C          │      5500 │         12.87 │           1 │                0.57 │ N                         │                          3 │\n",
       "│         23 │         65500 │ RENT                  │               4.0 │ MEDICAL     │ C          │     35000 │         15.23 │           1 │                0.53 │ N                         │                          2 │\n",
       "│         24 │         54400 │ RENT                  │               8.0 │ MEDICAL     │ C          │     35000 │         14.27 │           1 │                0.55 │ Y                         │                          4 │\n",
       "└────────────┴───────────────┴───────────────────────┴───────────────────┴─────────────┴────────────┴───────────┴───────────────┴─────────────┴─────────────────────┴───────────────────────────┴────────────────────────────┘"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "con.sql(\"SELECT * FROM reporting LIMIT 5;\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Altering Tables**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Add Columns**\n",
    "The ALTER TABLE statement is used to modify the columns of an existing table. When combined with the ADD clause, it is used to add a new column. \n",
    "\n",
    "In SQLite, you can add multiple columns to an existing table using the ALTER TABLE statement. However, SQLite does not support adding multiple columns in a single ALTER TABLE statement. Instead, you must execute separate ALTER TABLE commands for each column you want to add."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────┬────────────────┬───────┬─────────────────────┬────────────────────────────┬───────────┬─────────┐\n",
       "│  id   │      name      │  age  │     created_at      │           email            │ last_name │  test   │\n",
       "│ int32 │    varchar     │ int32 │      timestamp      │          varchar           │  varchar  │ varchar │\n",
       "├───────┼────────────────┼───────┼─────────────────────┼────────────────────────────┼───────────┼─────────┤\n",
       "│     0 │ John Vincent   │    52 │ 2025-05-19 22:28:17 │ john.vincent@example.com   │ NULL      │ NULL    │\n",
       "│     1 │ Jessica Carey  │    67 │ 2021-08-29 08:22:24 │ jessica.carey@example.com  │ NULL      │ NULL    │\n",
       "│     2 │ David Pratt    │    31 │ 2024-06-24 20:48:34 │ david.pratt@example.com    │ NULL      │ NULL    │\n",
       "│     3 │ Jennifer Allen │    69 │ 2024-10-12 15:37:12 │ jennifer.allen@example.com │ NULL      │ NULL    │\n",
       "│     4 │ Cory Baker     │    30 │ 2021-04-10 08:32:59 │ cory.baker@example.com     │ NULL      │ NULL    │\n",
       "└───────┴────────────────┴───────┴─────────────────────┴────────────────────────────┴───────────┴─────────┘"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    ALTER TABLE users\n",
    "    ADD COLUMN last_name TEXT;\n",
    "    \n",
    "    ALTER TABLE users\n",
    "    ADD COLUMN test TEXT;\n",
    "\"\"\"\n",
    "con.execute(query)\n",
    "\n",
    "con.sql(\"SELECT * FROM users LIMIT 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Delete Columns**\n",
    "We can also use the DROP COLUMN clause to remove a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "┌───────┬────────────────┬───────┬─────────────────────┬────────────────────────────┬───────────┐\n",
       "│  id   │      name      │  age  │     created_at      │           email            │ last_name │\n",
       "│ int32 │    varchar     │ int32 │      timestamp      │          varchar           │  varchar  │\n",
       "├───────┼────────────────┼───────┼─────────────────────┼────────────────────────────┼───────────┤\n",
       "│     0 │ John Vincent   │    52 │ 2025-05-19 22:28:17 │ john.vincent@example.com   │ NULL      │\n",
       "│     1 │ Jessica Carey  │    67 │ 2021-08-29 08:22:24 │ jessica.carey@example.com  │ NULL      │\n",
       "│     2 │ David Pratt    │    31 │ 2024-06-24 20:48:34 │ david.pratt@example.com    │ NULL      │\n",
       "│     3 │ Jennifer Allen │    69 │ 2024-10-12 15:37:12 │ jennifer.allen@example.com │ NULL      │\n",
       "│     4 │ Cory Baker     │    30 │ 2021-04-10 08:32:59 │ cory.baker@example.com     │ NULL      │\n",
       "└───────┴────────────────┴───────┴─────────────────────┴────────────────────────────┴───────────┘"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    ALTER TABLE users\n",
    "    DROP COLUMN test\n",
    "\"\"\"\n",
    "con.execute(query)\n",
    "\n",
    "con.sql(\"SELECT * FROM users LIMIT 5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Identify Constraints**\n",
    "\n",
    "Constraints are rules defined as part of the data model to control what values are allowed in specific columns and tables. Specifically, constraints reject inserts or updates containing values that shouldn’t be inserted into a database table, which can help with preserving data integrity and quality. They can also raise an error when they’re violated, which can help with debugging applications that write to the DB. \n",
    "\n",
    "Sometimes we’ve planned out a data model and inserted data before realizing that our model could benefit from the addition of a constraint. To find the constraints applied to a specific column, query the PostgreSQL catalog tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "conname",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "constraint_definition",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "column_name",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "a57ee3f2-595b-4977-b9f8-39e288bc850b",
       "rows": [
        [
         "140",
         "NOT NULL",
         "NOT NULL",
         "id"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>conname</th>\n",
       "      <th>constraint_definition</th>\n",
       "      <th>column_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>NOT NULL</td>\n",
       "      <td>NOT NULL</td>\n",
       "      <td>id</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      conname constraint_definition column_name\n",
       "140  NOT NULL              NOT NULL          id"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    SELECT conname, pg_get_constraintdef(c.oid) AS constraint_definition, a.attname AS column_name\n",
    "    FROM pg_constraint c\n",
    "    JOIN pg_class t ON c.conrelid = t.oid\n",
    "    JOIN pg_attribute a ON a.attnum = ANY(c.conkey)\n",
    "    WHERE t.relname = 'users';\n",
    "    \"\"\"\n",
    "\n",
    "df = con.sql(query).df()\n",
    "df[df.column_name == \"id\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Delete Constraints**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"\n",
    "#     ALTER TABLE users\n",
    "#     DROP CONSTRAINT users_email_key;\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Add Constraints**\n",
    "\n",
    "Constraints are rules defined as part of the data model to control what values are allowed in specific columns and tables. Specifically, constraints reject inserts or updates containing values that shouldn’t be inserted into a database table, which can help with preserving data integrity and quality. They can also raise an error when they’re violated, which can help with debugging applications that write to the DB. \n",
    "\n",
    "Sometimes we’ve planned out a data model and inserted data before realizing that our model could benefit from the addition of a constraint. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Not Null**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x22a67c49ff0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    ALTER TABLE users\n",
    "    ALTER COLUMN age SET NOT NULL;\n",
    "\"\"\"\n",
    "\n",
    "con.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deleting a NOT NULL constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x22a67c49ff0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    ALTER TABLE users\n",
    "    ALTER COLUMN age DROP NOT NULL;\n",
    "\"\"\"\n",
    "\n",
    "con.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Check**\n",
    "\n",
    "In some situations, we might want to establish specific rules to determine what makes a row valid. The condition tested for inside of parentheses of a CHECK statement must be a SQL statement that can be evaluated as either true or false. As a rule, any logic that you might use in a WHERE statement to filter individual rows from an existing table can be applied within a CHECK, including logic that involves multiple columns or conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"\n",
    "#     ALTER TABLE users\n",
    "#     ADD CHECK (age <= 100);\n",
    "# \"\"\"\n",
    "\n",
    "# con.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Single-column Unique**\n",
    "\n",
    "When designing a PostgreSQL data model, it’s a good practice to structure tables such that rows are uniquely identifiable by some combination of attributes. Identifying and implementing a PRIMARY KEY is easier on tables with UNIQUE constraints already in place. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"\n",
    "#     ALTER TABLE users\n",
    "#     ADD UNIQUE (email);\n",
    "# \"\"\"\n",
    "\n",
    "# con.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Multi-column Unique**\n",
    "\n",
    "We can create a multi-column UNIQUE constraint in the CREATE TABLE statement by specifying the columns that need to be jointly unique in parentheses on its own line following the column names and datatype definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"\n",
    "#     ALTER TABLE users\n",
    "#     ADD UNIQUE (email, phone);\n",
    "# \"\"\"\n",
    "\n",
    "# con.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Primary Key**\n",
    "\n",
    "Having unique constraints is useful, but an important part of building a relational data model requires defining relationships between tables. Primary keys are essential to defining these relationships. A primary key is a column (or set of columns) that uniquely identifies a row within a database table. A table can only have one primary key, and to be selected as a primary key a column (or set of columns) should:\n",
    "- Uniquely identify that row in the table (like a UNIQUE constraint)\n",
    "- Contain no null values (like a NOT NULL constraint)\n",
    "\n",
    "Implementing a PRIMARY KEY constraint is similar to simultaneously enforcing a UNIQUE and NOT NULL constraints on a column (or set of columns). Although UNIQUE NOT NULL and PRIMARY KEY constraints function very similarly, tables are limited to one PRIMARY KEY, but not limited in how many columns can have both UNIQUE and NOT NULL constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"\n",
    "#     ALTER TABLE users\n",
    "#     ADD PRIMARY KEY (id);\n",
    "# \"\"\"\n",
    "\n",
    "# con.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Composite Primary Key**\n",
    "\n",
    "Sometimes, none of the columns in a table can uniquely identify a record. When this happens, we can designate multiple columns in a table to serve as the primary key, also known as a composite primary key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"\n",
    "#     ALTER TABLE users\n",
    "#     ADD PRIMARY KEY (email, created_at);\n",
    "# \"\"\"\n",
    "\n",
    "# con.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Foreign Key**\n",
    "\n",
    "When discussing relations between tables, you may see the terms parent table and child table to describe two tables that are related. More specifically, values inserted into child table must be validated by data that’s already present in a parent table.\n",
    "Formally, this property that ensures data can be validated by referencing another table in the data model is called referential integrity. Referential integrity can be enforced by adding a FOREIGN KEY on the child table that references the primary key of a parent table.\n",
    "\n",
    "To designate a foreign key on a single column in PostgreSQL, we use the REFERENCES keyword:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x22a67c49ff0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    CREATE OR REPLACE TABLE storage (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    user_id INTEGER REFERENCES users(id),\n",
    "    space INTEGER,\n",
    "    price FLOAT\n",
    "    );\n",
    "\"\"\"\n",
    "con.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "user_id",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "space",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "price",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "e6b953ff-0ba6-4cab-b772-4dad39d08bdf",
       "rows": [
        [
         "0",
         "0",
         "58",
         "215",
         "3088"
        ],
        [
         "1",
         "1",
         "68",
         "32",
         "57759"
        ],
        [
         "2",
         "2",
         "87",
         "575",
         "21899"
        ],
        [
         "3",
         "3",
         "23",
         "856",
         "86126"
        ],
        [
         "4",
         "4",
         "83",
         "799",
         "44973"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>user_id</th>\n",
       "      <th>space</th>\n",
       "      <th>price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>58</td>\n",
       "      <td>215</td>\n",
       "      <td>3088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>68</td>\n",
       "      <td>32</td>\n",
       "      <td>57759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>87</td>\n",
       "      <td>575</td>\n",
       "      <td>21899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>23</td>\n",
       "      <td>856</td>\n",
       "      <td>86126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>83</td>\n",
       "      <td>799</td>\n",
       "      <td>44973</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  user_id  space  price\n",
       "0   0       58    215   3088\n",
       "1   1       68     32  57759\n",
       "2   2       87    575  21899\n",
       "3   3       23    856  86126\n",
       "4   4       83    799  44973"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dummy data set for storage\n",
    "storage_data = {\n",
    "    \"user_id\": [fake.random_int(min=1, max=99) for _ in range(sample)],\n",
    "    \"space\": [fake.random_int(min=1, max=1000) for _ in range(sample)],\n",
    "    \"price\": [fake.random_number(digits=5) for _ in range(sample)],\n",
    "}\n",
    "\n",
    "# Convert the dummy data to a DataFrame\n",
    "storage_df = pd.DataFrame(storage_data)\n",
    "storage_df.reset_index(inplace=True, names=\"id\")\n",
    "storage_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x22a67c49ff0>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    INSERT INTO storage BY NAME \n",
    "    SELECT * FROM storage_df\n",
    "\"\"\"\n",
    "\n",
    "con.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using an ALTER TABLE statement it would be this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"\n",
    "#     ALTER TABLE storage\n",
    "#     ADD FOREIGN KEY (user_id) REFERENCES users(id);\n",
    "# \"\"\"\n",
    "\n",
    "# con.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Cascade Constraint**\n",
    "\n",
    "By default, a foreign key constraint will prevent an engineer from deleting or updating a row of a parent table that is referenced by some child table. This behaviour is sometimes explicitly specified in a `CREATE TABLE` statement using `REFERENCES column_name(id) ON DELETE RESTRICT` or `REFERENCES column_name(id) ON UPDATE RESTRICT`. However, another strategy you may consider is adding a `CASCADE` clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"\n",
    "#     ALTER TABLE storage\n",
    "#     ADD FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE ON UPDATE CASCADE;\n",
    "# \"\"\"\n",
    "\n",
    "# con.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Indexing Tables**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An index is an organization of the data in a table to help with performance when searching and filtering records. A table can have zero, one, or many indexes. There are some costs when using indexes.\n",
    "\n",
    "By default, it divides the possible matching records in half, then half, then half, and so on until the specific match you are searching for is found. This is known as a Binary Tree, or B-Tree. In small databases this is negligible, but as the datasets get larger this becomes more significant. To highlight this, let us say you were searching 1,000,000 records. Without an index on the column you were searching, you would need to look through all 1,000,000 records (assuming its a non-unique column). With a B-Tree index, in the worst case, you would have to search 20 comparisons (log2n).\n",
    "\n",
    "Indexes speed up searching and filtering, however, they slow down insert, update, and delete statements. \n",
    "\n",
    "- When we add a record to a table that has an index, the index itself must be modified by the server as well. Recall that at its core, an index is an organization of the data in a table. When new data is added, the index will be reshaped to fit that new data into its organization. This means that when you write a single statement to modify the records, the server will have to modify every index that would be impacted by this change.\n",
    "- Updates and deletes have similar drawbacks. When deleting a record that is associated with an index, it might be faster to find the record — by leveraging the index’s ability to search. However, once the record is found, removing or editing it will result in the same issue as inserting a new record. The index itself will need to be redone. Note that if you’re updating a non-indexed column, that update will be unaffected by the index. So if you are updating a non-indexed column while filtering by one with an index, an update statement can actually be faster with an index.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Identify Indexes**\n",
    "\n",
    "PostgreSQL automatically creates a unique on any primary key you have in your tables. It will also do this for any column you define as having a unique constraint. A unique index, primary key, and unique constraint all reject any attempt to have two records in a table that would have the same value (multicolumns versions of these would reject any record where all the columns are equal).\n",
    "As a note, the primary key index standard is to end in _pkey instead of _idx to identify it as a specific type of index. It is also the way the system names it when created automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Existence**\n",
    "\n",
    "This function returns which indexes exist in your table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "schemaname",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tablename",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "indexname",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "tablespace",
         "rawType": "Int32",
         "type": "integer"
        },
        {
         "name": "indexdef",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "bf3f3159-6f5a-4805-8cc9-3347ced6be70",
       "rows": [
        [
         "0",
         "main",
         "users",
         "users_age_idx",
         null,
         "CREATE INDEX users_age_idx ON users(age);"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>schemaname</th>\n",
       "      <th>tablename</th>\n",
       "      <th>indexname</th>\n",
       "      <th>tablespace</th>\n",
       "      <th>indexdef</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>main</td>\n",
       "      <td>users</td>\n",
       "      <td>users_age_idx</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>CREATE INDEX users_age_idx ON users(age);</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  schemaname tablename      indexname  tablespace  \\\n",
       "0       main     users  users_age_idx        <NA>   \n",
       "\n",
       "                                    indexdef  \n",
       "0  CREATE INDEX users_age_idx ON users(age);  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM pg_indexes\n",
    "\"\"\"\n",
    "\n",
    "df = con.sql(query).df()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Size**\n",
    "The index data structures can sometimes take up as much space as the table itself. If you have not worked with large databases before you might be thinking to yourself, “who cares, storage space is cheap nowadays”. However, databases of decent size can easily get into the gigabyte size range quickly. If you wanted to examine the size of a table products you would run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"\n",
    "#     SELECT pg_size_pretty (pg_total_relation_size('users'));\n",
    "# \"\"\"\n",
    "# df = con.sql(query).df()\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Deleting Indexes**\n",
    "\n",
    "If you are adding a large amount of data to an existing table, it may be better to drop the index, add the data, and then recreate the index rather than having to update the index on each insertion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x22a67c49ff0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    DROP INDEX IF EXISTS users_age_idx;\n",
    "\"\"\"\n",
    "con.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating Indexes**\n",
    "\n",
    "Don’t forget the naming convention for indexes, `<table_name>_<column_name>_idx`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Single-column Indexes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x22a67c49ff0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    CREATE INDEX users_age_idx ON users(age);\n",
    "\"\"\"\n",
    "con.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Multi-column Indexes**\n",
    "\n",
    "Much like constraints, you can combine multiple columns together as a single index. When using multicolumn indexes, the search structure will be based on the values found in all the columns. For example, an index on First and Last Name might be a good idea if it is common to search by both together in your situation. Consider a table where the last names 'Smith' and 'Johnson' appear many times. Having another filter for the first name can help you find someone named 'Sarah Smith' much faster.\n",
    "\n",
    "The index is built in the specific order listed at creation, so (last_name, first_name) is different from (first_name, last_name). Keep this in mind when you are building your indexes as the order will impact the efficiency of your searches.\n",
    "Say you want to find 'David', 'Rachel', and 'Margaret' from the first_name column with the last_name of 'Smith'. If there is an index (last_name, first_name), the server would find everyone with the last name 'Smith' then in that much smaller group, find the specific first names you are searching for. If the index is (first_name, last_name) the server would go to each of the first_name records you are interested in and then search for the last name 'Smith' within each one. \n",
    "\n",
    "If there is a good use for it, you could create both indexes as well! If both are present, when you run your script, the database server will determine which index to use based on your query. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"\n",
    "#     CREATE INDEX reporting_product_type_customer_segment_idx ON reporting(product_type, customer_segment);\n",
    "# \"\"\"\n",
    "# con.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Partial Indexes**\n",
    "\n",
    "A partial index allows for indexing on a subset of a table, allowing searches to be conducted on just this group of records in the table. All you have to do is create an index like you normally would with a WHERE clause added on to specify the subgroup of data your index should encompass. Let’s assume that in our example the users are stored in a users table and we want an index based on `name`. Note that the filtering of the index does not have to be for a column that is part of your index. \n",
    "\n",
    "If we know that all internal employees have an email_address ending in '@wellsfargo.com', we would write the partial index like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"\n",
    "#     CREATE INDEX users_name_internal_idx ON users(name)\n",
    "#     WHERE email LIKE '%@wellsfargo.com';\n",
    "# \"\"\"\n",
    "# con.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Ordered Indexes**\n",
    "\n",
    "If you are commonly ordering your data in a specific way on an indexed column, you can add this information to the index itself and PostgreSQL will store the data in your desired order. By doing this, the results that are returned to you will already be sorted. You won’t need a second step of sorting them, saving time on your query.\n",
    "\n",
    "You could also use `ASC` to switch the direction. If your column contains `NULL`s you can also specify the order they appear by adding `NULLS FIRST` or `NULLS LAST` to fit your needs. By default, PostgreSQL orders indexes by ascending order with `NULL`s last, so if this is the order you desire, you do not need to do anything.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x22a67c49ff0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    CREATE INDEX users_date_ordered_idx ON users(created_at DESC NULLS FIRST);\n",
    "\"\"\"\n",
    "con.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Expression-based Indexes**\n",
    "\n",
    "An index is not limited to just a column reference, it can use the result of a function or scalar expression computed from one or more columns. \n",
    "\n",
    "In PostgreSQL, 'ExampleCompany' is NOT the same thing as 'examplecompany' even though we would probably want to reject this as a duplicate. You can add a function on your index to convert all your company_name data to lower case by using `LOWER`. This ensures that 'ExampleCompany' would be considered the same as 'examplecompany'. \n",
    "\n",
    "If you need flexibility, such as creating a unique index on a computed value or a subset of rows (e.g., partial indexes), create a unique index directly since creating a `UNIQUE` constraint on a table column directly does not allow this. A `UNIQUE` constraint applied to a column is a declarative constraint at the schema level, and PostgreSQL enforces it by automatically creating a unique index for that column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x22a67c49ff0>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    CREATE UNIQUE INDEX unique_users_name_idx ON users(LOWER(name));\n",
    "\"\"\"\n",
    "con.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Clustering Indexes**\n",
    "\n",
    "A clustered index is often tied to the table’s primary key. When a clustered index is created for a table, the data is physically organized in the table structure to allow for improved search times. \n",
    "\n",
    "You can think of the clustered index like searching a dictionary. In a dictionary, the data (words) and all their related information (definition) are physically ordered by their index (words sorted alphabetically). Just like a dictionary, you can seek your word by quickly jumping to the letter in the alphabet the word you’re looking for starts with. Because it is physically organizing the data in the table, there can only be one clustered index per table.\n",
    "\n",
    "When the system creates, alters, or refreshes a clustered index, it takes all the records in your database table that are in memory and rearranges them to match the order of your clustered index, physically altering their location in storage. Then when you go to do your searches for records based on this index, the system can use this index to find your records faster.\n",
    "\n",
    "Something to note that PostgreSQL does differently than other systems is that it does not maintain this order automatically. When inserting data into a table with a clustered index on other systems, those systems will place the new records and altered records in their correct location in the database order in memory. PostgreSQL keeps modified records where they are and adds new records to the end, regardless of sorting. If you want to maintain the order, you must run the CLUSTER command again on the index when there have been changes. This will “re-cluster” the index to put all of those new records in the correct place.\n",
    "\n",
    "Because PostgreSQL does not automatically recluster on `INSERT`, `UPDATE` and `DELETE` statements, those statements might run faster than equivalent statements using a different system. The flip side of this coin though is that after time, the more your table is modified the less useful the cluster will be on your searches. Reclustering the table has a cost, so you will need to find a balance on when to recluster your table(s). There are tools that can be used to help you identify when this would be useful, but these tools fall outside of this lesson.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To cluster your database table using an existing index (say products_product_name_idx) on the products table you would use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"\n",
    "#     CLUSTER reporting USING reporting_product_type_customer_segment_idx;\n",
    "# \"\"\"\n",
    "# con.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already established what index should be clustered on you can simply tell the system which table to apply the cluster on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"\n",
    "#     CLUSTER reporting;\n",
    "# \"\"\"\n",
    "# con.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if you want to cluster every table in your database that has an identified index to use you can simply call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query = \"\"\"\n",
    "#     CLUSTER;\n",
    "# \"\"\"\n",
    "# con.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Non-clustered Indexes**\n",
    "\n",
    "Previously we compared how a clustered index functions as a dictionary. You can think of all other indexes (non-clustered) more akin to an index in a book. The keywords you are looking for are organized (by type, alphabetically, by the number of appearances, etc) and can be found quickly. However, the index doesn’t contain information beyond that. Instead, it contains a pointer (page number, paragraph number, etc) to where the rest of the data can be found. This is the same way non-clustered indexes in databases work. You have a key that is sorted and a pointer to where to find the rest of the data if needed.\n",
    "\n",
    "PostgreSQL defaults all indexes to non-clustered unless you specify one to be clustered. So you do not need to do anything special when creating your index for it to be non-clustered.\n",
    "\n",
    "When you search on a non-clustered index for more information than is in the indexed columns, there are two searches. The first to find the record in the index and another to find the record the pointer identifies. There are some things you can do, such as creating a multicolumn index, that in some cases can help cut down or eliminate the need for the look back to the main table in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **Expression-based Indexes**\n",
    "\n",
    "An index is not limited to just a column reference, it can use the result of a function or scalar expression computed from one or more columns. \n",
    "\n",
    "In PostgreSQL, 'ExampleCompany' is NOT the same thing as 'examplecompany' even though we would probably want to reject this as a duplicate. You can add a function on your index to convert all your company_name data to lower case by using `LOWER`. This ensures that 'ExampleCompany' would be considered the same as 'examplecompany'. \n",
    "\n",
    "If you need flexibility, such as creating a unique index on a computed value or a subset of rows (e.g., partial indexes), create a unique index directly since creating a `UNIQUE` constraint on a table column directly does not allow this. A `UNIQUE` constraint applied to a column is a declarative constraint at the schema level, and PostgreSQL enforces it by automatically creating a unique index for that column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<duckdb.duckdb.DuckDBPyConnection at 0x22a67c49ff0>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "    CREATE UNIQUE INDEX unique_users_email_idx ON users(LOWER(email));\n",
    "\"\"\"\n",
    "con.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Deciding on Indexes**\n",
    "\n",
    "1. Stable tables: As a very rough rule of thumb, think carefully about any index on a table that gets regular Insert/Update/Delete.In contrast, a table that is fairly stable but is searched regularly might be a good candidate for an index.\n",
    "2. Needle in a haystack: The higher the percentage of a table you are returning the less useful an index becomes. If we’re only searching for 1 record in 1,000,000, an index could be incredibly useful. However, if we are searching for 900,000 out of that same 1,000,000 the advantages of an index become useless. At higher percentages, the query planner might completely ignore your index and do a full table scan, making your index only a burden on the system.\n",
    "3. AND is good, OR is bad: Along this same line, if you are combining filtering conditions be aware of what you will be searching on. AND statements are normally fine and the query planner will try to use an indexed field before non-indexed fields to cut down on the total number of records needed to be searched. OR on the other hand, can be very dangerous; even if you have a single non-indexed condition, if it’s in an OR, the system will still have to check every record in your table, making your index useless.\n",
    "4. Single multi-column vs Multiple single columns\n",
    "\n",
    "   - A multicolumn index is less efficient than a single index in cases where a single index is needed.\n",
    "   - A single multicolumn index is faster (if ordered well) than the server combining single indexes.\n",
    "   - You could create all of them (two single indexes and one multicolumn index), and then the server will try to use the best one in each case, but if they are all not used relatively often/equally then this is a misuse of indexes.\n",
    "\n",
    "    Take for example, searching for first_name and last_name in the users table.\n",
    "\n",
    "   - If searches are most often for only one of the columns, then you should use that single index. \n",
    "   - If searches are most often last_name and first_name, then you should have a multicolumn index. \n",
    "   - If the searches are frequent and evenly spread among first_name alone, last_name alone, and the combination of the two; that is a situation where you would want to have all three indexes (two single indexes and one multicolumn index).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Analyse Performance**\n",
    "\n",
    "To get insight into how PostgreSQL breaks down your statements into runnable parts, we can investigate the query plan by adding `EXPLAIN ANALYZE` before your query. Rather than returning the results of the query, it will return information about the query. For now, there are a few key things you should take note of. The first is the planner will specifically tell you how it is searching. If you see “Seq Scan” this means that the system is scanning every record to find the specific records you are looking for. If you see “Index” (in our examples more specifically “Bitmap Index Scan”) you know that the server is taking advantage of an index to improve the speed of your search.\n",
    "\n",
    "The other part to take note of is the “Planning time” and “Execution time”. The planning time is the amount of time the server spends deciding the best way to solve your query, should it use an index, or do a full scan of the table(s) for instance. The execution time is the amount of time the actual query takes to run after the server has decided on a plan of attack. You need to take both of these into consideration, and when examining your own indexes these are critical to understanding how effective your indexes are.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    EXPLAIN ANALYZE \n",
    "    SELECT * FROM users\n",
    "    WHERE age > 50;\n",
    "\"\"\"\n",
    "df = con.sql(query).df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ┌─────────────────────────────────────┐\n",
    "# │┌───────────────────────────────────┐│\n",
    "# ││    Query Profiling Information    ││\n",
    "# │└───────────────────────────────────┘│\n",
    "# └─────────────────────────────────────┘\n",
    "#      EXPLAIN ANALYZE      SELECT * FROM users     WHERE age > 50;\n",
    "# ┌────────────────────────────────────────────────┐\n",
    "# │┌──────────────────────────────────────────────┐│\n",
    "# ││              Total Time: 0.0007s             ││\n",
    "# │└──────────────────────────────────────────────┘│\n",
    "# └────────────────────────────────────────────────┘\n",
    "# ┌───────────────────────────┐\n",
    "# │           QUERY           │\n",
    "# └─────────────┬─────────────┘\n",
    "# ┌─────────────┴─────────────┐\n",
    "# │      EXPLAIN_ANALYZE      │\n",
    "# │    ────────────────────   │\n",
    "# │           0 Rows          │\n",
    "# │          (0.00s)          │\n",
    "# └─────────────┬─────────────┘\n",
    "# ┌─────────────┴─────────────┐\n",
    "# │         PROJECTION        │\n",
    "# │    ────────────────────   │\n",
    "# │             id            │\n",
    "# │            name           │\n",
    "# │            age            │\n",
    "# │         created_at        │\n",
    "# │           email           │\n",
    "# │                           │\n",
    "# │          46 Rows          │\n",
    "# │          (0.00s)          │\n",
    "# └─────────────┬─────────────┘\n",
    "# ┌─────────────┴─────────────┐\n",
    "# │         TABLE_SCAN        │\n",
    "# │    ────────────────────   │\n",
    "# │        Table: users       │\n",
    "# │      Type: Index Scan     │\n",
    "# │                           │\n",
    "# │        Projections:       │\n",
    "# │            age            │\n",
    "# │             id            │\n",
    "# │            name           │\n",
    "# │         created_at        │\n",
    "# │           email           │\n",
    "# │                           │\n",
    "# │      Filters: age>50      │\n",
    "# │                           │\n",
    "# │          46 Rows          │\n",
    "# │          (0.00s)          │\n",
    "# └───────────────────────────┘"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "guide-repo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
